{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c59d053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes:\n",
      "Train: (24545, 16)\n",
      "Val: (5260, 16)\n",
      "Test: (5260, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shipment_id</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>dispatch_date</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>delay_days</th>\n",
       "      <th>disruption_type</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>source</th>\n",
       "      <th>lead_time_days</th>\n",
       "      <th>delay_severity</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>route_risk_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O1000</td>\n",
       "      <td>B33</td>\n",
       "      <td>S23</td>\n",
       "      <td>2023-10-27 00:00:00</td>\n",
       "      <td>2023-10-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>resilience</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Minor</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2023</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O1001</td>\n",
       "      <td>B1</td>\n",
       "      <td>S20</td>\n",
       "      <td>2023-07-08 00:00:00</td>\n",
       "      <td>2023-07-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>resilience</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Minor</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2023</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O1002</td>\n",
       "      <td>B2</td>\n",
       "      <td>S10</td>\n",
       "      <td>2023-12-29 00:00:00</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Shortage</td>\n",
       "      <td>1.0</td>\n",
       "      <td>resilience</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Severe</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2023</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O1003</td>\n",
       "      <td>B6</td>\n",
       "      <td>S10</td>\n",
       "      <td>2023-01-17 00:00:00</td>\n",
       "      <td>2023-01-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>resilience</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  shipment_id origin destination        dispatch_date delivery_date  \\\n",
       "0       O1000    B33         S23  2023-10-27 00:00:00    2023-10-28   \n",
       "1       O1001     B1         S20  2023-07-08 00:00:00    2023-07-09   \n",
       "2       O1002     B2         S10  2023-12-29 00:00:00    2024-01-07   \n",
       "3       O1003     B6         S10  2023-01-17 00:00:00    2023-01-20   \n",
       "\n",
       "   delay_days disruption_type  risk_score      source  lead_time_days  \\\n",
       "0         0.0             NaN         0.0  resilience             1.0   \n",
       "1         0.0             NaN         0.0  resilience             1.0   \n",
       "2         7.0        Shortage         1.0  resilience             9.0   \n",
       "3         0.0             NaN         0.0  resilience             3.0   \n",
       "\n",
       "  delay_severity  month  weekday  quarter  year  route_risk_score  \n",
       "0          Minor     10        4        4  2023               1.0  \n",
       "1          Minor      7        5        3  2023               1.0  \n",
       "2         Severe     12        4        4  2023               1.0  \n",
       "3       Moderate      1        1        1  2023               1.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PROCESSED = \"../data/processed/tabular\"\n",
    "MODELS_DIR = \"../models\"\n",
    "RESULTS_DIR = \"../results/metrics\"\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Files produced by 06_split_data.ipynb\n",
    "TRAIN_PATH = f\"{DATA_PROCESSED}/train.csv\"\n",
    "VAL_PATH   = f\"{DATA_PROCESSED}/val.csv\"\n",
    "TEST_PATH  = f\"{DATA_PROCESSED}/test.csv\"\n",
    "\n",
    "# Load all datasets\n",
    "df_train = pd.read_csv(TRAIN_PATH, low_memory=False)\n",
    "df_val   = pd.read_csv(VAL_PATH, low_memory=False)\n",
    "df_test  = pd.read_csv(TEST_PATH, low_memory=False)\n",
    "\n",
    "print(\"Data shapes:\")\n",
    "print(f\"Train: {df_train.shape}\")\n",
    "print(f\"Val: {df_val.shape}\")\n",
    "print(f\"Test: {df_test.shape}\")\n",
    "\n",
    "df_train.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e48a43a",
   "metadata": {},
   "source": [
    "Utility: Feature/Target Auto-Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36f6b18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Multiple matches found: ['delay_days', 'lead_time_days', 'delay_severity']. Using the first one.\n",
      "⚠️ Multiple matches found: ['dispatch_date', 'delivery_date']. Using the first one.\n",
      "✅ Classification target: disruption_type\n",
      "✅ Regression target: delay_days\n",
      "✅ ID column: shipment_id\n",
      "✅ Date column: dispatch_date\n"
     ]
    }
   ],
   "source": [
    "def find_col(candidates, cols, col_map=None, return_all=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Return columns from candidates that exist in cols (case-insensitive).\n",
    "\n",
    "    Parameters:\n",
    "        candidates (list): List of candidate column names.\n",
    "        cols (list): List of columns to search in (lowercase).\n",
    "        col_map (dict, optional): Mapping from lowercase to original column names.\n",
    "        return_all (bool): If True, return all matches; else first match.\n",
    "        verbose (bool): If True, prints warnings for multiple matches.\n",
    "\n",
    "    Returns:\n",
    "        str or list: Matched column name(s) or None.\n",
    "    \"\"\"\n",
    "    matches = [c for c in candidates if c.lower() in cols]\n",
    "    if not matches:\n",
    "        return None\n",
    "    if return_all:\n",
    "        return [col_map[c.lower()] if col_map else c for c in matches]\n",
    "    if len(matches) > 1 and verbose:\n",
    "        print(f\"⚠️ Multiple matches found: {matches}. Using the first one.\")\n",
    "    return col_map[matches[0].lower()] if col_map else matches[0]\n",
    "\n",
    "\n",
    "# Lowercase lookup\n",
    "cols = [c.lower() for c in df_train.columns]\n",
    "col_map = {c.lower(): c for c in df_train.columns}\n",
    "\n",
    "# --- Classification target candidates ---\n",
    "classification_candidates = [\n",
    "    \"disruption_flag\",\"is_disrupted\",\"disrupted\",\"risk_flag\",\n",
    "    \"has_disruption\",\"disruption\",\"incident_flag\",\"disruption_type\"\n",
    "]\n",
    "clf_target = find_col(classification_candidates, cols, col_map=col_map)\n",
    "\n",
    "# --- Regression target candidates ---\n",
    "regression_candidates = [\n",
    "    \"delay_days\",\"delivery_delay_days\",\"delay\",\"days_delayed\",\n",
    "    \"delay_in_days\",\"lead_time_delays\",\"lead_time_days\",\"delay_severity\"\n",
    "]\n",
    "reg_target = find_col(regression_candidates, cols, col_map=col_map)\n",
    "\n",
    "# --- ID & Date candidates ---\n",
    "id_candidates = [\"shipment_id\",\"id\",\"order_id\",\"consignment_id\"]\n",
    "date_candidates = [\n",
    "    \"dispatch_date\",\"delivery_date\",\"ship_date\",\"event_time\",\n",
    "    \"timestamp\",\"created_at\",\"pickup_date\"\n",
    "]\n",
    "id_col   = find_col(id_candidates, cols, col_map=col_map)\n",
    "date_col = find_col(date_candidates, cols, col_map=col_map)\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"✅ Classification target:\", clf_target)\n",
    "print(\"✅ Regression target:\", reg_target)\n",
    "print(\"✅ ID column:\", id_col)\n",
    "print(\"✅ Date column:\", date_col)\n",
    "\n",
    "# --- Guardrails ---\n",
    "if clf_target is None and reg_target is None:\n",
    "    raise ValueError(\n",
    "        \"❌ No target columns detected. \"\n",
    "        \"Please set clf_target/reg_target manually.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f47c19",
   "metadata": {},
   "source": [
    "Split Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "968379b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classification target (train) extracted: disruption_type, shape: (24545,)\n",
      "✅ Classification target (val) extracted: disruption_type, shape: (5260,)\n",
      "✅ Classification target (test) extracted: disruption_type, shape: (5260,)\n",
      "✅ Regression target (train) extracted: delay_days, shape: (24545,)\n",
      "✅ Regression target (val) extracted: delay_days, shape: (5260,)\n",
      "✅ Regression target (test) extracted: delay_days, shape: (5260,)\n",
      "\n",
      "✅ Features prepared\n",
      "   Numerical columns (7): ['risk_score', 'lead_time_days', 'month', 'weekday', 'quarter']...\n",
      "   Categorical columns (5): ['origin', 'destination', 'delivery_date', 'source', 'delay_severity']\n",
      "   Regression target: delay_days\n",
      "   Classification target: disruption_type\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Safe Drop of Non-Feature Columns\n",
    "# ---------------------------\n",
    "drop_cols = {id_col, date_col, clf_target, reg_target} - {None}\n",
    "\n",
    "def safe_drop(df, drop_cols):\n",
    "    \"\"\"Drop columns safely if they exist in DataFrame.\"\"\"\n",
    "    return df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "X_train_full = safe_drop(df_train, drop_cols)\n",
    "X_val_full   = safe_drop(df_val, drop_cols)\n",
    "X_test_full  = safe_drop(df_test, drop_cols)\n",
    "\n",
    "# ---------------------------\n",
    "# Extract Targets\n",
    "# ---------------------------\n",
    "def extract_target(df, target_col, name=\"target\"):\n",
    "    \"\"\"Extract target column from DataFrame if exists.\"\"\"\n",
    "    if target_col and target_col in df.columns:\n",
    "        y = df[target_col].copy()\n",
    "        print(f\"✅ {name} extracted: {target_col}, shape: {y.shape}\")\n",
    "        return y\n",
    "    print(f\"⚠️ {name} not found in DataFrame.\")\n",
    "    return None\n",
    "\n",
    "# Classification targets\n",
    "y_train_clf = extract_target(df_train, clf_target, \"Classification target (train)\")\n",
    "y_val_clf   = extract_target(df_val, clf_target, \"Classification target (val)\")\n",
    "y_test_clf  = extract_target(df_test, clf_target, \"Classification target (test)\")\n",
    "\n",
    "# Regression targets\n",
    "y_train_reg = extract_target(df_train, reg_target, \"Regression target (train)\")\n",
    "y_val_reg   = extract_target(df_val, reg_target, \"Regression target (val)\")\n",
    "y_test_reg  = extract_target(df_test, reg_target, \"Regression target (test)\")\n",
    "\n",
    "# ---------------------------\n",
    "# Detect Feature Types\n",
    "# ---------------------------\n",
    "num_cols = [c for c in X_train_full.columns if pd.api.types.is_numeric_dtype(X_train_full[c])]\n",
    "cat_cols = [c for c in X_train_full.columns if c not in num_cols]\n",
    "\n",
    "print(\"\\n✅ Features prepared\")\n",
    "print(f\"   Numerical columns ({len(num_cols)}): {num_cols[:5]}{'...' if len(num_cols) > 5 else ''}\")\n",
    "print(f\"   Categorical columns ({len(cat_cols)}): {cat_cols[:5]}{'...' if len(cat_cols) > 5 else ''}\")\n",
    "print(f\"   Regression target: {reg_target}\")\n",
    "print(f\"   Classification target: {clf_target}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bb9b58",
   "metadata": {},
   "source": [
    "Common Preprocess Pipline (Impute + Scale + One-Hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e6b11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def robust_stratified_split(X, y, test_size=0.3, val_size=0.5, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits X, y into train/val/test with:\n",
    "    - Imputation for missing values\n",
    "    - Stratified splitting if possible\n",
    "    - Falls back to random split if only one class\n",
    "    \"\"\"\n",
    "    # ------------------------\n",
    "    # Drop rows where target is NaN\n",
    "    # ------------------------\n",
    "    mask = y.notna()\n",
    "    X, y = X.loc[mask].copy(), y.loc[mask].copy()\n",
    "\n",
    "    # ------------------------\n",
    "    # Impute missing features\n",
    "    # ------------------------\n",
    "    # Numeric\n",
    "    num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    if len(num_cols) > 0:\n",
    "        num_imputer = SimpleImputer(strategy=\"median\")\n",
    "        X.loc[:, num_cols] = num_imputer.fit_transform(X[num_cols])\n",
    "\n",
    "    # Categorical\n",
    "    cat_cols = X.select_dtypes(include=\"object\").columns\n",
    "    if len(cat_cols) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        X.loc[:, cat_cols] = cat_imputer.fit_transform(X[cat_cols])\n",
    "\n",
    "    # ------------------------\n",
    "    # Encode categorical target for stratification if needed\n",
    "    # ------------------------\n",
    "    if y.dtype == object or str(y.dtype) == 'category':\n",
    "        y_strat = y.astype(\"category\").cat.codes\n",
    "    else:\n",
    "        y_strat = y.copy()\n",
    "\n",
    "    # ------------------------\n",
    "    # Check if target has at least 2 classes\n",
    "    # ------------------------\n",
    "    unique_classes = np.unique(y_strat)\n",
    "    stratify_possible = len(unique_classes) > 1\n",
    "\n",
    "    if not stratify_possible:\n",
    "        print(\"⚠️ Only one class present. Using random split instead of stratified.\")\n",
    "\n",
    "    # ------------------------\n",
    "    # Stratified or random train/temp split\n",
    "    # ------------------------\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y_strat if stratify_possible else None,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # ------------------------\n",
    "    # Stratified or random val/test split\n",
    "    # ------------------------\n",
    "    y_temp_strat = y_temp.astype(\"category\").cat.codes if (stratify_possible and y_temp.dtype == object) else y_temp\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size, stratify=y_temp_strat if stratify_possible else None,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # ------------------------\n",
    "    # Log class distributions\n",
    "    # ------------------------\n",
    "    print(\"Train classes:\", np.unique(y_train, return_counts=True))\n",
    "    print(\"Val classes:\", np.unique(y_val, return_counts=True))\n",
    "    print(\"Test classes:\", np.unique(y_test, return_counts=True))\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b70256d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classes: (array(['Clear', 'Customs', 'Detour', 'Heavy', 'High Risk', 'Low Risk',\n",
      "       'Moderate Risk', 'Shortage', 'Strike', 'Weather'], dtype=object), array([  230,   174,   241,   229, 11213,  1486,  2382,   189,   171,\n",
      "         186]))\n",
      "Val classes: (array(['Clear', 'Customs', 'Detour', 'Heavy', 'High Risk', 'Low Risk',\n",
      "       'Moderate Risk', 'Shortage', 'Strike', 'Weather'], dtype=object), array([  49,   37,   52,   49, 2403,  319,  510,   41,   36,   40]))\n",
      "Test classes: (array(['Clear', 'Customs', 'Detour', 'Heavy', 'High Risk', 'Low Risk',\n",
      "       'Moderate Risk', 'Shortage', 'Strike', 'Weather'], dtype=object), array([  49,   37,   52,   49, 2403,  318,  511,   40,   37,   40]))\n",
      "✅ Train/Val/Test split done\n",
      "Train shape: (16501, 15) | Val shape: (3536, 15) | Test shape: (3536, 15)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Prepare features and target\n",
    "# ------------------------\n",
    "if clf_target in df_train.columns:\n",
    "    X = df_train.drop(columns=[clf_target], errors=\"ignore\").copy()\n",
    "    y = df_train[clf_target].copy()\n",
    "else:\n",
    "    print(f\"⚠️ Classification target '{clf_target}' not found in df_train. Skipping split.\")\n",
    "    X = df_train.copy()\n",
    "    y = None\n",
    "\n",
    "# ------------------------\n",
    "# Perform robust stratified split if target is valid\n",
    "# ------------------------\n",
    "if y is not None and y.notna().any():\n",
    "    try:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = robust_stratified_split(X, y)\n",
    "        \n",
    "        # ------------------------\n",
    "        # Reset indices for convenience\n",
    "        # ------------------------\n",
    "        for df in [X_train, X_val, X_test, y_train, y_val, y_test]:\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        print(\"✅ Train/Val/Test split done\")\n",
    "        print(\"Train shape:\", X_train.shape, \"| Val shape:\", X_val.shape, \"| Test shape:\", X_test.shape)\n",
    "    except ValueError as e:\n",
    "        print(f\"⚠️ Skipping split: {e}\")\n",
    "        X_train, X_val, X_test = X.copy(), None, None\n",
    "        y_train, y_val, y_test = y.copy() if y is not None else None, None, None\n",
    "else:\n",
    "    print(\"⚠️ Skipping split: no valid target detected.\")\n",
    "    X_train, X_val, X_test = X.copy(), None, None\n",
    "    y_train, y_val, y_test = y.copy() if y is not None else None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bbdb2892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Setting up consistent preprocessing...\n",
      "✅ Preprocessor configuration:\n",
      "   Numeric columns (5): ['risk_score', 'month', 'weekday']...\n",
      "   Special numeric columns (2): ['lead_time_days', 'route_risk_score']\n",
      "   Special categorical columns (0): []\n",
      "   Flag columns (0): []\n",
      "   General categorical columns (5): ['origin', 'destination', 'delivery_date']...\n",
      "🔧 Fitting preprocessor on full training data...\n",
      "🔧 Transforming datasets...\n",
      "✅ Processed dataset shapes:\n",
      "   Training: (16501, 1751)\n",
      "   Validation: (3536, 1751)\n",
      "   Test: (3536, 1751)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Consistent Preprocessing for Delay Classification\n",
    "# ---------------------------\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "clf_delay_results = {}\n",
    "\n",
    "if reg_target is not None:\n",
    "    \n",
    "    # ---------------------------\n",
    "    # FIT PREPROCESSOR ON FULL TRAINING DATA FIRST\n",
    "    # ---------------------------\n",
    "    print(\"🔧 Setting up consistent preprocessing...\")\n",
    "    \n",
    "    # Column Groups (using your existing logic)\n",
    "    X_cols = X_train.columns if X_train is not None else []\n",
    "    \n",
    "    # Special categorical\n",
    "    special_cat_cols = [c for c in [\"disruption_type\"] if c in X_cols]\n",
    "    \n",
    "    # Flag columns (binary indicators)\n",
    "    flag_cols = [c for c in X_cols if c.lower() in [\"risk_flag\",\"disruption_flag\",\"incident_flag\"]]\n",
    "    \n",
    "    # Special numeric columns\n",
    "    special_num_cols = [c for c in X_cols if c.lower() in [\"lead_time_days\", \"route_risk_score\"]]\n",
    "    \n",
    "    # General numeric (exclude special numeric, flags, regression target)\n",
    "    num_cols = [c for c in X_train.select_dtypes(include=[\"int64\",\"float64\"]).columns \n",
    "                if c not in special_num_cols + flag_cols + ([reg_target] if reg_target else [])]\n",
    "    \n",
    "    # General categorical (exclude special categorical, flags, ID/date)\n",
    "    non_features = [id_col, date_col] if 'id_col' in globals() and 'date_col' in globals() else []\n",
    "    cat_cols = [c for c in X_train.select_dtypes(include=\"object\").columns \n",
    "                if c not in special_cat_cols + flag_cols + [col for col in non_features if col]]\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Create Pipelines\n",
    "    # ---------------------------\n",
    "    num_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ]) if num_cols else None\n",
    "    \n",
    "    special_num_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ]) if special_num_cols else None\n",
    "    \n",
    "    special_cat_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"none\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True, drop='first'))  # Add drop='first' to reduce features\n",
    "    ]) if special_cat_cols else None\n",
    "    \n",
    "    flag_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0))\n",
    "    ]) if flag_cols else None\n",
    "    \n",
    "    general_cat_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True, drop='first'))  # Add drop='first' to reduce features\n",
    "    ]) if cat_cols else None\n",
    "    \n",
    "    # ---------------------------\n",
    "    # ColumnTransformer\n",
    "    # ---------------------------\n",
    "    transformers = []\n",
    "    if num_cols and num_pipeline: transformers.append((\"num\", num_pipeline, num_cols))\n",
    "    if special_num_cols and special_num_pipeline: transformers.append((\"special_num\", special_num_pipeline, special_num_cols))\n",
    "    if special_cat_cols and special_cat_pipeline: transformers.append((\"special_cat\", special_cat_pipeline, special_cat_cols))\n",
    "    if flag_cols and flag_pipeline: transformers.append((\"flags\", flag_pipeline, flag_cols))\n",
    "    if cat_cols and general_cat_pipeline: transformers.append((\"general_cat\", general_cat_pipeline, cat_cols))\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.3\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Preprocessor configuration:\")\n",
    "    print(f\"   Numeric columns ({len(num_cols)}): {num_cols[:3]}{'...' if len(num_cols) > 3 else ''}\")\n",
    "    print(f\"   Special numeric columns ({len(special_num_cols)}): {special_num_cols}\")\n",
    "    print(f\"   Special categorical columns ({len(special_cat_cols)}): {special_cat_cols}\")\n",
    "    print(f\"   Flag columns ({len(flag_cols)}): {flag_cols}\")\n",
    "    print(f\"   General categorical columns ({len(cat_cols)}): {cat_cols[:3]}{'...' if len(cat_cols) > 3 else ''}\")\n",
    "    \n",
    "    # ---------------------------\n",
    "    # FIT PREPROCESSOR ON FULL TRAINING DATA\n",
    "    # ---------------------------\n",
    "    print(\"🔧 Fitting preprocessor on full training data...\")\n",
    "    preprocessor.fit(X_train)\n",
    "    \n",
    "    # Transform all datasets using the fitted preprocessor\n",
    "    print(\"🔧 Transforming datasets...\")\n",
    "    X_train_processed = preprocessor.transform(X_train)\n",
    "    X_val_processed = preprocessor.transform(X_val)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Convert to dense if needed (for easier handling)\n",
    "    if hasattr(X_train_processed, 'toarray'):\n",
    "        X_train_processed = X_train_processed.toarray()\n",
    "        X_val_processed = X_val_processed.toarray()\n",
    "        X_test_processed = X_test_processed.toarray()\n",
    "    \n",
    "    print(f\"✅ Processed dataset shapes:\")\n",
    "    print(f\"   Training: {X_train_processed.shape}\")\n",
    "    print(f\"   Validation: {X_val_processed.shape}\")\n",
    "    print(f\"   Test: {X_test_processed.shape}\")\n",
    "    \n",
    "    # Verify all datasets have the same number of features\n",
    "    if not (X_train_processed.shape[1] == X_val_processed.shape[1] == X_test_processed.shape[1]):\n",
    "        raise ValueError(f\"Feature count mismatch after preprocessing: \"\n",
    "                        f\"Train={X_train_processed.shape[1]}, \"\n",
    "                        f\"Val={X_val_processed.shape[1]}, \"\n",
    "                        f\"Test={X_test_processed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cfeca3",
   "metadata": {},
   "source": [
    "Classification: Risk Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39c95017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Merging rare classes into 'Other': ['Detour', 'Clear', 'Heavy', 'Shortage', 'Weather', 'Customs', 'Strike']\n",
      "Detected target 'disruption_type' with 4 unique classes.\n",
      "✅ Multi-class classification detected.\n",
      "✅ Class weights for Logistic Regression: {'High Risk': np.float64(0.36789886738606975), 'Low Risk': np.float64(2.7760767160161506), 'Moderate Risk': np.float64(1.7318429890848026), 'Other': np.float64(2.9051056338028167)}\n",
      "🔹 Training logreg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 5000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=5000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training rf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classification results saved with class imbalance handling!\n",
      "  logreg: F1-macro = 0.7505935893630877, Test AUC = 0.893349486214937\n",
      "  rf: F1-macro = 0.3719721145157785, Test AUC = 1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "clf_results = {}\n",
    "\n",
    "# -------------------------------\n",
    "# Proceed only if classification target exists\n",
    "# -------------------------------\n",
    "if clf_target is not None and y_train is not None:\n",
    "\n",
    "    # -------------------------------\n",
    "    # Merge rare classes\n",
    "    # -------------------------------\n",
    "    class_counts = y_train.value_counts()\n",
    "    rare_classes = class_counts[class_counts < 1000].index.tolist()  # adjust threshold as needed\n",
    "    if rare_classes:\n",
    "        print(f\"⚠️ Merging rare classes into 'Other': {rare_classes}\")\n",
    "        y_train_proc = y_train.replace(rare_classes, \"Other\")\n",
    "        y_val_proc = y_val.replace(rare_classes, \"Other\")\n",
    "        y_test_proc = y_test.replace(rare_classes, \"Other\")\n",
    "    else:\n",
    "        y_train_proc, y_val_proc, y_test_proc = y_train.copy(), y_val.copy(), y_test.copy()\n",
    "\n",
    "    # -------------------------------\n",
    "    # Detect if binary or multi-class\n",
    "    # -------------------------------\n",
    "    classes = np.unique(y_train_proc)\n",
    "    is_binary = len(classes) == 2\n",
    "    print(f\"Detected target '{clf_target}' with {len(classes)} unique classes.\")\n",
    "    print(f\"✅ {'Binary' if is_binary else 'Multi-class'} classification detected.\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # Compute class weights for Logistic Regression\n",
    "    # -------------------------------\n",
    "    class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train_proc)\n",
    "    class_weight_dict = {cls: w for cls, w in zip(classes, class_weights)}\n",
    "    print(f\"✅ Class weights for Logistic Regression: {class_weight_dict}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # Ensure all splits have >1 class\n",
    "    # -------------------------------\n",
    "    skip_classification = any(len(np.unique(y)) < 2 for y in [y_train_proc, y_val_proc, y_test_proc])\n",
    "    if skip_classification:\n",
    "        print(\"⚠️ Skipping classification due to insufficient class diversity in splits.\")\n",
    "    else:\n",
    "\n",
    "        # -------------------------------\n",
    "        # Define pipelines\n",
    "        # -------------------------------\n",
    "        logreg = Pipeline([\n",
    "            (\"prep\", preprocessor),\n",
    "            (\"clf\", LogisticRegression(\n",
    "                max_iter=5000,\n",
    "                solver=\"lbfgs\",\n",
    "                multi_class=\"multinomial\",\n",
    "                class_weight=class_weight_dict\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        rf = Pipeline([\n",
    "            (\"prep\", preprocessor),\n",
    "            (\"clf\", RandomForestClassifier(\n",
    "                n_estimators=300,\n",
    "                max_depth=15,\n",
    "                min_samples_leaf=5,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        models = {\"logreg\": logreg, \"rf\": rf}\n",
    "\n",
    "        # -------------------------------\n",
    "        # Train, evaluate, and save models\n",
    "        # -------------------------------\n",
    "        for name, pipe in models.items():\n",
    "            print(f\"🔹 Training {name}...\")\n",
    "            pipe.fit(X_train, y_train_proc)\n",
    "\n",
    "            # Predictions\n",
    "            test_pred = pipe.predict(X_test)\n",
    "            val_pred = pipe.predict(X_val)\n",
    "\n",
    "            # Classification report & confusion matrix\n",
    "            report = classification_report(y_test_proc, test_pred, output_dict=True, zero_division=0)\n",
    "            cm = confusion_matrix(y_test_proc, test_pred).tolist()\n",
    "            f1_macro = f1_score(y_test_proc, test_pred, average=\"macro\")\n",
    "\n",
    "            # Multi-class AUC\n",
    "            try:\n",
    "                val_pred_prob = pipe.predict_proba(X_val)\n",
    "                test_pred_prob = pipe.predict_proba(X_test)\n",
    "                val_auc = roc_auc_score(y_val_proc, val_pred_prob, multi_class='ovr')\n",
    "                test_auc = roc_auc_score(y_test_proc, test_pred_prob, multi_class='ovr')\n",
    "            except Exception as e:\n",
    "                val_auc, test_auc = None, None\n",
    "                print(f\"⚠️ Could not compute multi-class AUC for {name}: {e}\")\n",
    "\n",
    "            clf_results[name] = {\n",
    "                \"f1_macro\": float(f1_macro),\n",
    "                \"val_auc\": float(val_auc) if val_auc is not None else None,\n",
    "                \"test_auc\": float(test_auc) if test_auc is not None else None,\n",
    "                \"report\": report,\n",
    "                \"cm\": cm\n",
    "            }\n",
    "\n",
    "            # Save model\n",
    "            os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "            joblib.dump(pipe, os.path.join(MODELS_DIR, f\"clf_{name}.joblib\"))\n",
    "\n",
    "        # Save metrics\n",
    "        os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "        with open(os.path.join(RESULTS_DIR, \"classification_results.json\"), \"w\") as f:\n",
    "            json.dump(clf_results, f, indent=2)\n",
    "\n",
    "        print(\"✅ Classification results saved with class imbalance handling!\")\n",
    "        for k, v in clf_results.items():\n",
    "            print(f\"  {k}: F1-macro = {v['f1_macro']}, Test AUC = {v['test_auc']}\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Skipping classification: no valid classification target detected or target is empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3e6ebd",
   "metadata": {},
   "source": [
    "Delay Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9dddc454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Original class distribution:\n",
      "delay_days\n",
      "Minor delay        3348\n",
      "Moderate delay     3884\n",
      "On-time            5262\n",
      "Severe delay      12051\n",
      "Name: count, dtype: int64\n",
      "🔹 Original class counts: {'Severe delay': np.int64(7641), 'On-time': np.int64(3822), 'Minor delay': np.int64(2546), 'Moderate delay': np.int64(2492)}\n",
      "🔹 Target size per class: 2492\n",
      "✅ Class counts after undersampling:\n",
      "Minor delay       2492\n",
      "Moderate delay    2492\n",
      "On-time           2492\n",
      "Severe delay      2492\n",
      "Name: count, dtype: int64\n",
      "✅ Encoded classes (multi-class): {'Minor delay': np.int64(0), 'Moderate delay': np.int64(1), 'On-time': np.int64(2), 'Severe delay': np.int64(3)}\n",
      "✅ Encoded classes (binary): {np.str_('Not Severe'): np.int64(0), np.str_('Severe'): np.int64(1)}\n",
      "\n",
      "=== Multi-class classification ===\n",
      "🔹 Training logreg (multi-class)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 5000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=5000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Validation accuracy: 0.188\n",
      "   Test accuracy: 0.181\n",
      "🔹 Training rf (multi-class)...\n",
      "   Validation accuracy: 0.251\n",
      "   Test accuracy: 0.236\n",
      "🔹 Training xgb (multi-class)...\n",
      "   Validation accuracy: 0.258\n",
      "   Test accuracy: 0.260\n",
      "\n",
      "=== Binary classification (Severe vs Not Severe) ===\n",
      "🔹 Training logreg (binary)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1262: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Validation accuracy: 0.447\n",
      "   Test accuracy: 0.459\n",
      "🔹 Training rf (binary)...\n",
      "   Validation accuracy: 0.447\n",
      "   Test accuracy: 0.459\n",
      "🔹 Training xgb (binary)...\n",
      "   Validation accuracy: 0.454\n",
      "   Test accuracy: 0.464\n",
      "\n",
      "=== SUMMARY OF RESULTS ===\n",
      "🏆 Best multi model: xgb (F1: 0.235, Acc: 0.260)\n",
      "🏆 Best binary model: xgb (F1: 0.345, Acc: 0.464)\n",
      "✅ Delay classification and model saving completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Improved Delay Classification with Model Saving\n",
    "# ---------------------------\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import joblib  # <-- for saving models\n",
    "\n",
    "clf_delay_results = {}\n",
    "best_models = {}  # To save best models\n",
    "\n",
    "if reg_target is not None:\n",
    "\n",
    "    # ---------------------------\n",
    "    # Bucketize delays into classes\n",
    "    # ---------------------------\n",
    "    def bucketize_delay(y):\n",
    "        y = pd.to_numeric(y, errors=\"coerce\")\n",
    "        if pd.isna(y):\n",
    "            return \"On-time\"\n",
    "        y = max(y, 0)\n",
    "        if y == 0:\n",
    "            return \"On-time\"\n",
    "        elif 1 <= y <= 3:\n",
    "            return \"Minor delay\"\n",
    "        elif 4 <= y <= 7:\n",
    "            return \"Moderate delay\"\n",
    "        else:\n",
    "            return \"Severe delay\"\n",
    "\n",
    "    y_train_c = y_train_reg.apply(bucketize_delay)\n",
    "    y_val_c   = y_val_reg.apply(bucketize_delay)\n",
    "    y_test_c  = y_test_reg.apply(bucketize_delay)\n",
    "\n",
    "    print(\"🔹 Original class distribution:\")\n",
    "    print(y_train_c.value_counts().sort_index())\n",
    "\n",
    "    # ---------------------------\n",
    "    # Align X and y\n",
    "    # ---------------------------\n",
    "    def align_features_targets(X, y):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "        common_idx = X.index.intersection(y.index)\n",
    "        return X.loc[common_idx], y.loc[common_idx]\n",
    "\n",
    "    X_train_aligned, y_train_aligned = align_features_targets(X_train_processed, y_train_c)\n",
    "    X_val_aligned, y_val_aligned     = align_features_targets(X_val_processed, y_val_c)\n",
    "    X_test_aligned, y_test_aligned   = align_features_targets(X_test_processed, y_test_c)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Improved undersampling\n",
    "    # ---------------------------\n",
    "    def undersample_balanced(X, y, strategy='uniform', random_state=42):\n",
    "        df = X.copy()\n",
    "        df['target'] = y\n",
    "        class_counts = y.value_counts()\n",
    "        print(f\"🔹 Original class counts: {dict(class_counts)}\")\n",
    "        \n",
    "        target_size = min(3000, class_counts.min()) if strategy=='uniform' else class_counts.min()\n",
    "        print(f\"🔹 Target size per class: {target_size}\")\n",
    "        \n",
    "        balanced_dfs = []\n",
    "        for cls in class_counts.index:\n",
    "            cls_df = df[df['target']==cls]\n",
    "            if len(cls_df) > target_size:\n",
    "                cls_df = resample(cls_df, replace=False, n_samples=target_size, random_state=random_state)\n",
    "            balanced_dfs.append(cls_df)\n",
    "        \n",
    "        df_balanced = pd.concat(balanced_dfs).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "        X_bal = df_balanced.drop(columns=['target']).values\n",
    "        y_bal = df_balanced['target'].values\n",
    "        return X_bal, y_bal\n",
    "\n",
    "    X_train_balanced, y_train_balanced = undersample_balanced(X_train_aligned, y_train_aligned)\n",
    "    print(\"✅ Class counts after undersampling:\")\n",
    "    print(pd.Series(y_train_balanced).value_counts().sort_index())\n",
    "\n",
    "    # ---------------------------\n",
    "    # Multi-class encoding\n",
    "    # ---------------------------\n",
    "    delay_le = LabelEncoder()\n",
    "    y_train_enc = delay_le.fit_transform(y_train_balanced)\n",
    "    y_val_enc   = delay_le.transform(y_val_aligned)\n",
    "    y_test_enc  = delay_le.transform(y_test_aligned)\n",
    "    print(\"✅ Encoded classes (multi-class):\", dict(zip(delay_le.classes_, delay_le.transform(delay_le.classes_))))\n",
    "\n",
    "    # ---------------------------\n",
    "    # Binary collapse (Severe vs Not-Severe)\n",
    "    # ---------------------------\n",
    "    def collapse_binary(y):\n",
    "        return np.where(y==\"Severe delay\", \"Severe\", \"Not Severe\")\n",
    "\n",
    "    y_train_bin = collapse_binary(y_train_balanced)\n",
    "    y_val_bin   = collapse_binary(y_val_aligned)\n",
    "    y_test_bin  = collapse_binary(y_test_aligned)\n",
    "\n",
    "    bin_le = LabelEncoder()\n",
    "    y_train_bin_enc = bin_le.fit_transform(y_train_bin)\n",
    "    y_val_bin_enc   = bin_le.transform(y_val_bin)\n",
    "    y_test_bin_enc  = bin_le.transform(y_test_bin)\n",
    "    print(\"✅ Encoded classes (binary):\", dict(zip(bin_le.classes_, bin_le.transform(bin_le.classes_))))\n",
    "\n",
    "    # ---------------------------\n",
    "    # Define models\n",
    "    # ---------------------------\n",
    "    def get_models(num_classes):\n",
    "        return {\n",
    "            \"logreg\": LogisticRegression(max_iter=5000, solver='lbfgs', multi_class='multinomial', random_state=42),\n",
    "            \"rf\": RandomForestClassifier(n_estimators=300, max_depth=15, min_samples_split=10,\n",
    "                                         min_samples_leaf=5, random_state=42, n_jobs=-1),\n",
    "            \"xgb\": XGBClassifier(\n",
    "                objective=\"multi:softprob\" if num_classes>2 else \"binary:logistic\",\n",
    "                eval_metric=\"mlogloss\",\n",
    "                n_estimators=300,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=0.1,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                verbosity=0\n",
    "            )\n",
    "        }\n",
    "\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    MODELS_DIR = os.path.join(RESULTS_DIR, \"models\")\n",
    "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Multi-class classification\n",
    "    # ---------------------------\n",
    "    print(\"\\n=== Multi-class classification ===\")\n",
    "    models_mc = get_models(len(delay_le.classes_))\n",
    "    for name, model in models_mc.items():\n",
    "        print(f\"🔹 Training {name} (multi-class)...\")\n",
    "        try:\n",
    "            model.fit(X_train_balanced, y_train_enc)\n",
    "            val_pred_enc = model.predict(X_val_aligned)\n",
    "            test_pred_enc = model.predict(X_test_aligned)\n",
    "            val_pred = delay_le.inverse_transform(val_pred_enc)\n",
    "            test_pred = delay_le.inverse_transform(test_pred_enc)\n",
    "\n",
    "            val_report = classification_report(y_val_aligned, val_pred, output_dict=True, zero_division=0)\n",
    "            test_report = classification_report(y_test_aligned, test_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "            clf_delay_results[f\"{name}_multi\"] = {\n",
    "                \"val_report\": val_report,\n",
    "                \"test_report\": test_report,\n",
    "                \"confusion_matrix\": confusion_matrix(y_test_aligned, test_pred, labels=delay_le.classes_).tolist(),\n",
    "                \"classes\": list(delay_le.classes_)\n",
    "            }\n",
    "\n",
    "            # Save best model by F1\n",
    "            test_f1 = test_report['macro avg']['f1-score']\n",
    "            if 'multi' not in best_models or test_f1 > best_models['multi']['f1']:\n",
    "                best_models['multi'] = {'name': name, 'f1': test_f1, 'accuracy': test_report['accuracy'], 'model': model}\n",
    "                joblib.dump(model, os.path.join(MODELS_DIR, f\"best_delay_multi_{name}.joblib\"))\n",
    "\n",
    "            print(f\"   Validation accuracy: {val_report['accuracy']:.3f}\")\n",
    "            print(f\"   Test accuracy: {test_report['accuracy']:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error training {name}: {str(e)}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Binary classification\n",
    "    # ---------------------------\n",
    "    print(\"\\n=== Binary classification (Severe vs Not Severe) ===\")\n",
    "    models_bin = get_models(len(bin_le.classes_))\n",
    "    for name, model in models_bin.items():\n",
    "        print(f\"🔹 Training {name} (binary)...\")\n",
    "        try:\n",
    "            model.fit(X_train_balanced, y_train_bin_enc)\n",
    "            val_pred_enc = model.predict(X_val_aligned)\n",
    "            test_pred_enc = model.predict(X_test_aligned)\n",
    "            val_pred = bin_le.inverse_transform(val_pred_enc)\n",
    "            test_pred = bin_le.inverse_transform(test_pred_enc)\n",
    "\n",
    "            val_report = classification_report(y_val_bin, val_pred, output_dict=True, zero_division=0)\n",
    "            test_report = classification_report(y_test_bin, test_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "            clf_delay_results[f\"{name}_binary\"] = {\n",
    "                \"val_report\": val_report,\n",
    "                \"test_report\": test_report,\n",
    "                \"confusion_matrix\": confusion_matrix(y_test_bin, test_pred, labels=bin_le.classes_).tolist(),\n",
    "                \"classes\": list(bin_le.classes_)\n",
    "            }\n",
    "\n",
    "            # Save best model by F1\n",
    "            test_f1 = test_report['macro avg']['f1-score']\n",
    "            if 'binary' not in best_models or test_f1 > best_models['binary']['f1']:\n",
    "                best_models['binary'] = {'name': name, 'f1': test_f1, 'accuracy': test_report['accuracy'], 'model': model}\n",
    "                joblib.dump(model, os.path.join(MODELS_DIR, f\"best_delay_binary_{name}.joblib\"))\n",
    "\n",
    "            print(f\"   Validation accuracy: {val_report['accuracy']:.3f}\")\n",
    "            print(f\"   Test accuracy: {test_report['accuracy']:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error training {name}: {str(e)}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Save results JSON\n",
    "    # ---------------------------\n",
    "    with open(os.path.join(RESULTS_DIR, \"delay_classification_results.json\"), \"w\") as f:\n",
    "        json.dump(clf_delay_results, f, indent=2)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Print summary\n",
    "    # ---------------------------\n",
    "    print(\"\\n=== SUMMARY OF RESULTS ===\")\n",
    "    for model_type, info in best_models.items():\n",
    "        print(f\"🏆 Best {model_type} model: {info['name']} (F1: {info['f1']:.3f}, Acc: {info['accuracy']:.3f})\")\n",
    "    \n",
    "    print(\"✅ Delay classification and model saving completed successfully!\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Skipping delay classification: no regression target detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0423a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multi-class classification ===\n",
      "🔹 Training logreg (multi-class)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 5000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=5000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training rf (multi-class)...\n",
      "🔹 Training xgb (multi-class)...\n",
      "\n",
      "=== Binary classification (Severe vs Not Severe) ===\n",
      "🔹 Training logreg (binary)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training rf (binary)...\n",
      "🔹 Training xgb (binary)...\n",
      "✅ Saved best multi model: ../results/metrics\\models\\best_delay_smotemulti.joblib\n",
      "✅ Saved best binary model: ../results/metrics\\models\\best_delay_smotebinary.joblib\n",
      "\n",
      "✅ Delay classification completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Delay Classification with SMOTE & Best Model Saving\n",
    "# ---------------------------\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "if reg_target is not None:\n",
    "\n",
    "    # ---------------------------\n",
    "    # Bucketize delays\n",
    "    # ---------------------------\n",
    "    def bucketize_delay(y):\n",
    "        y = pd.to_numeric(y, errors=\"coerce\")\n",
    "        if pd.isna(y) or y == 0:\n",
    "            return \"On-time\"\n",
    "        elif 1 <= y <= 3:\n",
    "            return \"Minor delay\"\n",
    "        elif 4 <= y <= 7:\n",
    "            return \"Moderate delay\"\n",
    "        else:\n",
    "            return \"Severe delay\"\n",
    "\n",
    "    y_train_c = y_train_reg.apply(bucketize_delay)\n",
    "    y_val_c   = y_val_reg.apply(bucketize_delay)\n",
    "    y_test_c  = y_test_reg.apply(bucketize_delay)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Align X and y\n",
    "    # ---------------------------\n",
    "    def align_features_targets(X, y):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "        common_idx = X.index.intersection(y.index)\n",
    "        return X.loc[common_idx], y.loc[common_idx]\n",
    "\n",
    "    X_train_aligned, y_train_aligned = align_features_targets(X_train_processed, y_train_c)\n",
    "    X_val_aligned, y_val_aligned     = align_features_targets(X_val_processed, y_val_c)\n",
    "    X_test_aligned, y_test_aligned   = align_features_targets(X_test_processed, y_test_c)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Multi-class encoding with SMOTE\n",
    "    # ---------------------------\n",
    "    delay_le = LabelEncoder()\n",
    "    y_train_enc = delay_le.fit_transform(y_train_aligned)\n",
    "\n",
    "    # Apply SMOTE for multi-class\n",
    "    smote_mc = SMOTE(random_state=42)\n",
    "    X_train_bal, y_train_bal = smote_mc.fit_resample(X_train_aligned, y_train_enc)\n",
    "\n",
    "    y_val_enc = delay_le.transform(y_val_aligned)\n",
    "    y_test_enc = delay_le.transform(y_test_aligned)\n",
    "\n",
    "    # Binary collapse (Severe vs Not-Severe)\n",
    "    def collapse_binary(y_raw):\n",
    "        return np.where(y_raw == \"Severe delay\", \"Severe\", \"Not Severe\")\n",
    "\n",
    "    y_train_bin = collapse_binary(y_train_aligned)\n",
    "    y_val_bin   = collapse_binary(y_val_aligned)\n",
    "    y_test_bin  = collapse_binary(y_test_aligned)\n",
    "\n",
    "    bin_le = LabelEncoder()\n",
    "    y_train_bin_enc = bin_le.fit_transform(y_train_bin)\n",
    "    y_val_bin_enc   = bin_le.transform(y_val_bin)\n",
    "    y_test_bin_enc  = bin_le.transform(y_test_bin)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Define models\n",
    "    # ---------------------------\n",
    "    def get_models(num_classes):\n",
    "        return {\n",
    "            \"logreg\": LogisticRegression(max_iter=5000, solver='lbfgs',\n",
    "                                         multi_class='multinomial' if num_classes>2 else 'auto',\n",
    "                                         random_state=42),\n",
    "            \"rf\": RandomForestClassifier(n_estimators=200, max_depth=15,\n",
    "                                         min_samples_split=10, min_samples_leaf=5,\n",
    "                                         random_state=42, n_jobs=-1),\n",
    "            \"xgb\": XGBClassifier(\n",
    "                objective=\"multi:softprob\" if num_classes > 2 else \"binary:logistic\",\n",
    "                eval_metric=\"mlogloss\",\n",
    "                n_estimators=300,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=0.1,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                verbosity=0\n",
    "            )\n",
    "        }\n",
    "\n",
    "    clf_delay_results = {}\n",
    "    best_models = {}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Train Multi-class models\n",
    "    # ---------------------------\n",
    "    print(\"=== Multi-class classification ===\")\n",
    "    models_mc = get_models(len(delay_le.classes_))\n",
    "    for name, model in models_mc.items():\n",
    "        print(f\"🔹 Training {name} (multi-class)...\")\n",
    "        model.fit(X_train_bal, y_train_bal)\n",
    "        val_pred_enc = model.predict(X_val_aligned)\n",
    "        test_pred_enc = model.predict(X_test_aligned)\n",
    "        val_pred = delay_le.inverse_transform(val_pred_enc)\n",
    "        test_pred = delay_le.inverse_transform(test_pred_enc)\n",
    "\n",
    "        val_report = classification_report(y_val_aligned, val_pred, output_dict=True, zero_division=0)\n",
    "        test_report = classification_report(y_test_aligned, test_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "        clf_delay_results[f\"{name}_multi\"] = {\n",
    "            \"val_report\": val_report,\n",
    "            \"test_report\": test_report,\n",
    "            \"confusion_matrix\": confusion_matrix(y_test_aligned, test_pred, labels=delay_le.classes_).tolist(),\n",
    "            \"classes\": list(delay_le.classes_)\n",
    "        }\n",
    "\n",
    "        f1_test = test_report['macro avg']['f1-score']\n",
    "        if \"multi\" not in best_models or f1_test > best_models[\"multi\"][\"f1\"]:\n",
    "            best_models[\"multi\"] = {\"name\": name, \"f1\": f1_test, \"model\": model}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Train Binary models\n",
    "    # ---------------------------\n",
    "    print(\"\\n=== Binary classification (Severe vs Not Severe) ===\")\n",
    "    models_bin = get_models(len(bin_le.classes_))\n",
    "    for name, model in models_bin.items():\n",
    "        print(f\"🔹 Training {name} (binary)...\")\n",
    "        model.fit(X_train_aligned, y_train_bin_enc)\n",
    "        val_pred_enc = model.predict(X_val_aligned)\n",
    "        test_pred_enc = model.predict(X_test_aligned)\n",
    "        val_pred = bin_le.inverse_transform(val_pred_enc)\n",
    "        test_pred = bin_le.inverse_transform(test_pred_enc)\n",
    "\n",
    "        val_report = classification_report(y_val_bin, val_pred, output_dict=True, zero_division=0)\n",
    "        test_report = classification_report(y_test_bin, test_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "        clf_delay_results[f\"{name}_binary\"] = {\n",
    "            \"val_report\": val_report,\n",
    "            \"test_report\": test_report,\n",
    "            \"confusion_matrix\": confusion_matrix(y_test_bin, test_pred, labels=bin_le.classes_).tolist(),\n",
    "            \"classes\": list(bin_le.classes_)\n",
    "        }\n",
    "\n",
    "        f1_test = test_report['macro avg']['f1-score']\n",
    "        if \"binary\" not in best_models or f1_test > best_models[\"binary\"][\"f1\"]:\n",
    "            best_models[\"binary\"] = {\"name\": name, \"f1\": f1_test, \"model\": model}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Save results and best models\n",
    "    # ---------------------------\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    with open(os.path.join(RESULTS_DIR, \"delay_classification_results_smote.json\"), \"w\") as f:\n",
    "        json.dump(clf_delay_results, f, indent=2)\n",
    "\n",
    "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "    for key, info in best_models.items():\n",
    "        save_path = os.path.join(MODELS_DIR, f\"best_delay_smote{key}.joblib\")\n",
    "        joblib.dump(info[\"model\"], save_path)\n",
    "        print(f\"✅ Saved best {key} model: {save_path}\")\n",
    "\n",
    "    print(\"\\n✅ Delay classification completed successfully!\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping delay classification: no regression target detected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9dd12",
   "metadata": {},
   "source": [
    "Anomaly Detection (IsolationForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "659c0185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Anomaly detection summary: {'threshold_percentile': 5, 'threshold_value': -0.5977612512504518, 'anomalies_detected': 177, 'total_test': 3536, 'rate': 0.05005656108597285}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def run_isolation_forest(X_train, X_test, num_cols, RESULTS_DIR=\"results\", MODELS_DIR=\"models\", percentile=5):\n",
    "    if len(num_cols) == 0:\n",
    "        print(\"⚠️ Skipping anomaly detection: no numeric columns found.\")\n",
    "        return None, None\n",
    "\n",
    "    # Pipeline for numeric columns + IsolationForest\n",
    "    num_pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "\n",
    "    iso_pipeline = Pipeline([\n",
    "        (\"num_only\", ColumnTransformer([(\"num\", num_pipeline, num_cols)], remainder=\"drop\")),\n",
    "        (\"iso\", IsolationForest(\n",
    "            n_estimators=300,\n",
    "            contamination=\"auto\",\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Fit pipeline\n",
    "    iso_pipeline.fit(X_train)\n",
    "\n",
    "    # Score test samples\n",
    "    test_scores = iso_pipeline[\"iso\"].score_samples(iso_pipeline[\"num_only\"].transform(X_test))\n",
    "\n",
    "    # Determine anomaly threshold\n",
    "    anomaly_threshold = np.percentile(test_scores, percentile)\n",
    "    anomalies = (test_scores <= anomaly_threshold).astype(int)\n",
    "\n",
    "    anomaly_report = {\n",
    "        \"threshold_percentile\": percentile,\n",
    "        \"threshold_value\": float(anomaly_threshold),\n",
    "        \"anomalies_detected\": int(anomalies.sum()),\n",
    "        \"total_test\": int(len(anomalies)),\n",
    "        \"rate\": float(anomalies.mean())\n",
    "    }\n",
    "\n",
    "    # Save results\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    with open(os.path.join(RESULTS_DIR, \"anomaly_results.json\"), \"w\") as f:\n",
    "        json.dump(anomaly_report, f, indent=2)\n",
    "\n",
    "    # Save trained pipeline\n",
    "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "    joblib.dump({\n",
    "        \"pipeline\": iso_pipeline,\n",
    "        \"threshold\": anomaly_threshold\n",
    "    }, os.path.join(MODELS_DIR, \"anomaly_isolation_forest.joblib\"))\n",
    "\n",
    "    print(\"✅ Anomaly detection summary:\", anomaly_report)\n",
    "\n",
    "    # Return anomalies and report\n",
    "    return anomalies, anomaly_report\n",
    "\n",
    "# Example usage\n",
    "anomalies, anomaly_report = run_isolation_forest(X_train, X_test, num_cols, RESULTS_DIR=RESULTS_DIR, MODELS_DIR=MODELS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b8804a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model card saved.\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# 7) Feature Importance (Optional, Tree-based models)\n",
    "# ========================================================\n",
    "def export_rf_importance(pipeline, out_json):\n",
    "    \"\"\"\n",
    "    Export feature importances from RandomForest model\n",
    "    after preprocessing (numeric + categorical features).\n",
    "    \"\"\"\n",
    "    if \"prep\" not in pipeline.named_steps:\n",
    "        print(\"⚠️ Pipeline has no 'prep' step. Skipping feature importance.\")\n",
    "        return\n",
    "\n",
    "    ct = pipeline.named_steps[\"prep\"]\n",
    "\n",
    "    # Find classifier step\n",
    "    rf_step = [k for k in pipeline.named_steps if \"clf\" in k or \"reg\" in k]\n",
    "    if not rf_step:\n",
    "        print(\"⚠️ No RandomForest step found in pipeline. Skipping.\")\n",
    "        return\n",
    "    rf = pipeline.named_steps[rf_step[0]]\n",
    "\n",
    "    # ✅ Get feature names directly from the ColumnTransformer\n",
    "    try:\n",
    "        full_features = ct.get_feature_names_out()\n",
    "    except:\n",
    "        # fallback if version <1.0\n",
    "        num_features = list(ct.transformers_[0][2])\n",
    "        if \"cat\" in ct.named_transformers_:\n",
    "            cat_ohe = ct.named_transformers_[\"cat\"].named_steps.get(\"cat_ohe\", None)\n",
    "            if cat_ohe:\n",
    "                cat_features = list(cat_ohe.get_feature_names_out(ct.transformers_[1][2]))\n",
    "            else:\n",
    "                cat_features = []\n",
    "        else:\n",
    "            cat_features = []\n",
    "        full_features = num_features + cat_features\n",
    "\n",
    "    importances = getattr(rf, \"feature_importances_\", None)\n",
    "    if importances is None:\n",
    "        print(\"⚠️ Model has no feature_importances_. Skipping.\")\n",
    "        return\n",
    "\n",
    "    if len(full_features) != len(importances):\n",
    "        print(f\"⚠️ Mismatch: {len(full_features)} features vs {len(importances)} importances. Aligning...\")\n",
    "        min_len = min(len(full_features), len(importances))\n",
    "        full_features = full_features[:min_len]\n",
    "        importances = importances[:min_len]\n",
    "\n",
    "    imp_df = pd.DataFrame({\"feature\": full_features, \"importance\": importances})\n",
    "    imp_df = imp_df.sort_values(\"importance\", ascending=False).head(50)\n",
    "\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    imp_df.to_json(out_json, orient=\"records\", indent=2)\n",
    "\n",
    "    print(f\"✅ Top feature importances saved to {out_json}\")\n",
    "    display(imp_df.head(20))\n",
    "\n",
    "\n",
    "\n",
    "# ========================================================\n",
    "# 8) Save a Compact “Model Card”\n",
    "# ========================================================\n",
    "from datetime import datetime\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "model_card = {\n",
    "    \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"train_rows\": int(len(df_train)),\n",
    "    \"val_rows\": int(len(df_val)),\n",
    "    \"test_rows\": int(len(df_test)),\n",
    "    \"targets\": {\n",
    "        \"classification\": clf_target,\n",
    "        \"anomaly_detection\": True\n",
    "    },\n",
    "    \"models_trained\": list(clf_results.keys()) + [\"isolation_forest\"],\n",
    "    \"paths\": {\n",
    "        \"models_dir\": MODELS_DIR,\n",
    "        \"results_dir\": RESULTS_DIR\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(RESULTS_DIR, \"model_card.json\"), \"w\") as f:\n",
    "    json.dump(model_card, f, indent=2)\n",
    "\n",
    "print(\"✅ Model card saved.\")\n",
    "\n",
    "\n",
    "# ========================================================\n",
    "# 9) Inference Helpers (for Backend)\n",
    "# ========================================================\n",
    "def load_model(path):\n",
    "    \"\"\"\n",
    "    Load a joblib-saved model/pipeline.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Model not found: {path}\")\n",
    "    return joblib.load(path)\n",
    "\n",
    "\n",
    "def predict_disruption(model_path, df_batch: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Predict disruption type (classification).\n",
    "    Returns predicted classes and probabilities.\n",
    "    \"\"\"\n",
    "    model = load_model(model_path)\n",
    "    preds = model.predict(df_batch)\n",
    "\n",
    "    probs = getattr(model, \"predict_proba\", None)\n",
    "    if callable(probs):\n",
    "        probs = model.predict_proba(df_batch)\n",
    "        classes = model.classes_.tolist()\n",
    "    else:\n",
    "        probs = None\n",
    "        classes = None\n",
    "\n",
    "    return {\"predictions\": preds.tolist(), \"probabilities\": probs.tolist() if probs is not None else None, \"classes\": classes}\n",
    "\n",
    "\n",
    "def score_anomaly(model_path, df_batch: pd.DataFrame, percentile: float = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Score anomalies using IsolationForest.\n",
    "    Returns anomaly scores and binary anomaly flags.\n",
    "    \"\"\"\n",
    "    pipe = load_model(model_path)\n",
    "    if \"num_only\" not in pipe.named_steps or \"iso\" not in pipe.named_steps:\n",
    "        raise ValueError(\"Pipeline must contain 'num_only' and 'iso' steps\")\n",
    "\n",
    "    X = pipe[\"num_only\"].transform(df_batch)\n",
    "    scores = pipe[\"iso\"].score_samples(X)\n",
    "    threshold = np.percentile(scores, percentile)\n",
    "    flags = (scores <= threshold).astype(int)\n",
    "\n",
    "    return {\"scores\": scores.tolist(), \"anomaly_flags\": flags.tolist(), \"threshold\": float(threshold)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a470034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
