{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c59d053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (24545, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shipment_id</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>dispatch_date</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>delay_days</th>\n",
       "      <th>disruption_type</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>source</th>\n",
       "      <th>lead_time_days</th>\n",
       "      <th>delay_severity</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>route_risk_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O1000</td>\n",
       "      <td>B33</td>\n",
       "      <td>S23</td>\n",
       "      <td>2023-10-27 00:00:00</td>\n",
       "      <td>2023-10-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>resilience</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Minor</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2023</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O1001</td>\n",
       "      <td>B1</td>\n",
       "      <td>S20</td>\n",
       "      <td>2023-07-08 00:00:00</td>\n",
       "      <td>2023-07-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>resilience</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Minor</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2023</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O1002</td>\n",
       "      <td>B2</td>\n",
       "      <td>S10</td>\n",
       "      <td>2023-12-29 00:00:00</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Shortage</td>\n",
       "      <td>1.0</td>\n",
       "      <td>resilience</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Severe</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2023</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O1003</td>\n",
       "      <td>B6</td>\n",
       "      <td>S10</td>\n",
       "      <td>2023-01-17 00:00:00</td>\n",
       "      <td>2023-01-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>resilience</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  shipment_id origin destination        dispatch_date delivery_date  \\\n",
       "0       O1000    B33         S23  2023-10-27 00:00:00    2023-10-28   \n",
       "1       O1001     B1         S20  2023-07-08 00:00:00    2023-07-09   \n",
       "2       O1002     B2         S10  2023-12-29 00:00:00    2024-01-07   \n",
       "3       O1003     B6         S10  2023-01-17 00:00:00    2023-01-20   \n",
       "\n",
       "   delay_days disruption_type  risk_score      source  lead_time_days  \\\n",
       "0         0.0             NaN         0.0  resilience             1.0   \n",
       "1         0.0             NaN         0.0  resilience             1.0   \n",
       "2         7.0        Shortage         1.0  resilience             9.0   \n",
       "3         0.0             NaN         0.0  resilience             3.0   \n",
       "\n",
       "  delay_severity  month  weekday  quarter  year  route_risk_score  \n",
       "0          Minor     10        4        4  2023               1.0  \n",
       "1          Minor      7        5        3  2023               1.0  \n",
       "2         Severe     12        4        4  2023               1.0  \n",
       "3       Moderate      1        1        1  2023               1.0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PROCESSED = \"../data/processed/tabular\"\n",
    "MODELS_DIR = \"../models\"\n",
    "RESULTS_DIR = \"../results/metrics\"\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_PATH = f\"{DATA_PROCESSED}/train.csv\"\n",
    "\n",
    "df = pd.read_csv(TRAIN_PATH, low_memory=False)\n",
    "print(\"Data shape:\", df.shape)\n",
    "df.head(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e48a43a",
   "metadata": {},
   "source": [
    "Utility: Feature/Target Auto-Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "36f6b18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Multiple matches found: ['delay_days', 'lead_time_days', 'delay_severity']. Using the first one.\n",
      "⚠️ Multiple matches found: ['dispatch_date', 'delivery_date']. Using the first one.\n",
      "✅ Classification target: disruption_type\n",
      "✅ Regression target: delay_days\n",
      "✅ ID column: shipment_id\n",
      "✅ Date column: dispatch_date\n"
     ]
    }
   ],
   "source": [
    "def find_col(candidates, cols, col_map=None, return_all=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Return columns from candidates that exist in cols (case-insensitive).\n",
    "\n",
    "    Parameters:\n",
    "        candidates (list): List of candidate column names.\n",
    "        cols (list): List of columns to search in (lowercase).\n",
    "        col_map (dict, optional): Mapping from lowercase to original column names.\n",
    "        return_all (bool): If True, return all matches; else first match.\n",
    "        verbose (bool): If True, prints warnings for multiple matches.\n",
    "\n",
    "    Returns:\n",
    "        str or list: Matched column name(s) or None.\n",
    "    \"\"\"\n",
    "    matches = [c for c in candidates if c.lower() in cols]\n",
    "    if not matches:\n",
    "        return None\n",
    "    if return_all:\n",
    "        return [col_map[c.lower()] if col_map else c for c in matches]\n",
    "    if len(matches) > 1 and verbose:\n",
    "        print(f\"⚠️ Multiple matches found: {matches}. Using the first one.\")\n",
    "    return col_map[matches[0].lower()] if col_map else matches[0]\n",
    "\n",
    "\n",
    "# Lowercase lookup\n",
    "cols = [c.lower() for c in df_train.columns]\n",
    "col_map = {c.lower(): c for c in df_train.columns}\n",
    "\n",
    "# --- Classification target candidates ---\n",
    "classification_candidates = [\n",
    "    \"disruption_flag\",\"is_disrupted\",\"disrupted\",\"risk_flag\",\n",
    "    \"has_disruption\",\"disruption\",\"incident_flag\",\"disruption_type\"\n",
    "]\n",
    "clf_target = find_col(classification_candidates, cols, col_map=col_map)\n",
    "\n",
    "# --- Regression target candidates ---\n",
    "regression_candidates = [\n",
    "    \"delay_days\",\"delivery_delay_days\",\"delay\",\"days_delayed\",\n",
    "    \"delay_in_days\",\"lead_time_delays\",\"lead_time_days\",\"delay_severity\"\n",
    "]\n",
    "reg_target = find_col(regression_candidates, cols, col_map=col_map)\n",
    "\n",
    "# --- ID & Date candidates ---\n",
    "id_candidates = [\"shipment_id\",\"id\",\"order_id\",\"consignment_id\"]\n",
    "date_candidates = [\n",
    "    \"dispatch_date\",\"delivery_date\",\"ship_date\",\"event_time\",\n",
    "    \"timestamp\",\"created_at\",\"pickup_date\"\n",
    "]\n",
    "id_col   = find_col(id_candidates, cols, col_map=col_map)\n",
    "date_col = find_col(date_candidates, cols, col_map=col_map)\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"✅ Classification target:\", clf_target)\n",
    "print(\"✅ Regression target:\", reg_target)\n",
    "print(\"✅ ID column:\", id_col)\n",
    "print(\"✅ Date column:\", date_col)\n",
    "\n",
    "# --- Guardrails ---\n",
    "if clf_target is None and reg_target is None:\n",
    "    raise ValueError(\n",
    "        \"❌ No target columns detected. \"\n",
    "        \"Please set clf_target/reg_target manually.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f47c19",
   "metadata": {},
   "source": [
    "Split Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "968379b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classification target (train) extracted: disruption_type, shape: (24545,)\n",
      "✅ Classification target (val) extracted: disruption_type, shape: (5260,)\n",
      "✅ Classification target (test) extracted: disruption_type, shape: (5260,)\n",
      "✅ Regression target (train) extracted: delay_days, shape: (24545,)\n",
      "✅ Regression target (val) extracted: delay_days, shape: (5260,)\n",
      "✅ Regression target (test) extracted: delay_days, shape: (5260,)\n",
      "\n",
      "✅ Features prepared\n",
      "   Numerical columns (7): ['risk_score', 'lead_time_days', 'month', 'weekday', 'quarter']...\n",
      "   Categorical columns (5): ['origin', 'destination', 'delivery_date', 'source', 'delay_severity']\n",
      "   Regression target: delay_days\n",
      "   Classification target: disruption_type\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Safe Drop of Non-Feature Columns\n",
    "# ---------------------------\n",
    "drop_cols = {id_col, date_col, clf_target, reg_target} - {None}\n",
    "\n",
    "def safe_drop(df, drop_cols):\n",
    "    \"\"\"Drop columns safely if they exist in DataFrame.\"\"\"\n",
    "    return df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "X_train_full = safe_drop(df_train, drop_cols)\n",
    "X_val_full   = safe_drop(df_val, drop_cols)\n",
    "X_test_full  = safe_drop(df_test, drop_cols)\n",
    "\n",
    "# ---------------------------\n",
    "# Extract Targets\n",
    "# ---------------------------\n",
    "def extract_target(df, target_col, name=\"target\"):\n",
    "    \"\"\"Extract target column from DataFrame if exists.\"\"\"\n",
    "    if target_col and target_col in df.columns:\n",
    "        y = df[target_col].copy()\n",
    "        print(f\"✅ {name} extracted: {target_col}, shape: {y.shape}\")\n",
    "        return y\n",
    "    print(f\"⚠️ {name} not found in DataFrame.\")\n",
    "    return None\n",
    "\n",
    "# Classification targets\n",
    "y_train_clf = extract_target(df_train, clf_target, \"Classification target (train)\")\n",
    "y_val_clf   = extract_target(df_val, clf_target, \"Classification target (val)\")\n",
    "y_test_clf  = extract_target(df_test, clf_target, \"Classification target (test)\")\n",
    "\n",
    "# Regression targets\n",
    "y_train_reg = extract_target(df_train, reg_target, \"Regression target (train)\")\n",
    "y_val_reg   = extract_target(df_val, reg_target, \"Regression target (val)\")\n",
    "y_test_reg  = extract_target(df_test, reg_target, \"Regression target (test)\")\n",
    "\n",
    "# ---------------------------\n",
    "# Detect Feature Types\n",
    "# ---------------------------\n",
    "num_cols = [c for c in X_train_full.columns if pd.api.types.is_numeric_dtype(X_train_full[c])]\n",
    "cat_cols = [c for c in X_train_full.columns if c not in num_cols]\n",
    "\n",
    "print(\"\\n✅ Features prepared\")\n",
    "print(f\"   Numerical columns ({len(num_cols)}): {num_cols[:5]}{'...' if len(num_cols) > 5 else ''}\")\n",
    "print(f\"   Categorical columns ({len(cat_cols)}): {cat_cols[:5]}{'...' if len(cat_cols) > 5 else ''}\")\n",
    "print(f\"   Regression target: {reg_target}\")\n",
    "print(f\"   Classification target: {clf_target}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bb9b58",
   "metadata": {},
   "source": [
    "Common Preprocess Pipline (Impute + Scale + One-Hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6e6b11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def robust_stratified_split(X, y, test_size=0.3, val_size=0.5, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits X, y into train/val/test with:\n",
    "    - Imputation for missing values\n",
    "    - Stratified splitting if possible\n",
    "    - Falls back to random split if only one class\n",
    "    \"\"\"\n",
    "    # ------------------------\n",
    "    # Drop rows where target is NaN\n",
    "    # ------------------------\n",
    "    mask = y.notna()\n",
    "    X, y = X.loc[mask].copy(), y.loc[mask].copy()\n",
    "\n",
    "    # ------------------------\n",
    "    # Impute missing features\n",
    "    # ------------------------\n",
    "    # Numeric\n",
    "    num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    if len(num_cols) > 0:\n",
    "        num_imputer = SimpleImputer(strategy=\"median\")\n",
    "        X.loc[:, num_cols] = num_imputer.fit_transform(X[num_cols])\n",
    "\n",
    "    # Categorical\n",
    "    cat_cols = X.select_dtypes(include=\"object\").columns\n",
    "    if len(cat_cols) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        X.loc[:, cat_cols] = cat_imputer.fit_transform(X[cat_cols])\n",
    "\n",
    "    # ------------------------\n",
    "    # Encode categorical target for stratification if needed\n",
    "    # ------------------------\n",
    "    if y.dtype == object or str(y.dtype) == 'category':\n",
    "        y_strat = y.astype(\"category\").cat.codes\n",
    "    else:\n",
    "        y_strat = y.copy()\n",
    "\n",
    "    # ------------------------\n",
    "    # Check if target has at least 2 classes\n",
    "    # ------------------------\n",
    "    unique_classes = np.unique(y_strat)\n",
    "    stratify_possible = len(unique_classes) > 1\n",
    "\n",
    "    if not stratify_possible:\n",
    "        print(\"⚠️ Only one class present. Using random split instead of stratified.\")\n",
    "\n",
    "    # ------------------------\n",
    "    # Stratified or random train/temp split\n",
    "    # ------------------------\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y_strat if stratify_possible else None,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # ------------------------\n",
    "    # Stratified or random val/test split\n",
    "    # ------------------------\n",
    "    y_temp_strat = y_temp.astype(\"category\").cat.codes if (stratify_possible and y_temp.dtype == object) else y_temp\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size, stratify=y_temp_strat if stratify_possible else None,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # ------------------------\n",
    "    # Log class distributions\n",
    "    # ------------------------\n",
    "    print(\"Train classes:\", np.unique(y_train, return_counts=True))\n",
    "    print(\"Val classes:\", np.unique(y_val, return_counts=True))\n",
    "    print(\"Test classes:\", np.unique(y_test, return_counts=True))\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b70256d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classes: (array(['Clear', 'Customs', 'Detour', 'Heavy', 'High Risk', 'Low Risk',\n",
      "       'Moderate Risk', 'Shortage', 'Strike', 'Weather'], dtype=object), array([  230,   174,   241,   229, 11213,  1486,  2382,   189,   171,\n",
      "         186]))\n",
      "Val classes: (array(['Clear', 'Customs', 'Detour', 'Heavy', 'High Risk', 'Low Risk',\n",
      "       'Moderate Risk', 'Shortage', 'Strike', 'Weather'], dtype=object), array([  49,   37,   52,   49, 2403,  319,  510,   41,   36,   40]))\n",
      "Test classes: (array(['Clear', 'Customs', 'Detour', 'Heavy', 'High Risk', 'Low Risk',\n",
      "       'Moderate Risk', 'Shortage', 'Strike', 'Weather'], dtype=object), array([  49,   37,   52,   49, 2403,  318,  511,   40,   37,   40]))\n",
      "✅ Train/Val/Test split done\n",
      "Train shape: (16501, 15) | Val shape: (3536, 15) | Test shape: (3536, 15)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Prepare features and target\n",
    "# ------------------------\n",
    "if clf_target in df_train.columns:\n",
    "    X = df_train.drop(columns=[clf_target], errors=\"ignore\").copy()\n",
    "    y = df_train[clf_target].copy()\n",
    "else:\n",
    "    print(f\"⚠️ Classification target '{clf_target}' not found in df_train. Skipping split.\")\n",
    "    X = df_train.copy()\n",
    "    y = None\n",
    "\n",
    "# ------------------------\n",
    "# Perform robust stratified split if target is valid\n",
    "# ------------------------\n",
    "if y is not None and y.notna().any():\n",
    "    try:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = robust_stratified_split(X, y)\n",
    "        \n",
    "        # ------------------------\n",
    "        # Reset indices for convenience\n",
    "        # ------------------------\n",
    "        for df in [X_train, X_val, X_test, y_train, y_val, y_test]:\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        print(\"✅ Train/Val/Test split done\")\n",
    "        print(\"Train shape:\", X_train.shape, \"| Val shape:\", X_val.shape, \"| Test shape:\", X_test.shape)\n",
    "    except ValueError as e:\n",
    "        print(f\"⚠️ Skipping split: {e}\")\n",
    "        X_train, X_val, X_test = X.copy(), None, None\n",
    "        y_train, y_val, y_test = y.copy() if y is not None else None, None, None\n",
    "else:\n",
    "    print(\"⚠️ Skipping split: no valid target detected.\")\n",
    "    X_train, X_val, X_test = X.copy(), None, None\n",
    "    y_train, y_val, y_test = y.copy() if y is not None else None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bbdb2892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessor ready.\n",
      "   Numeric columns: ['risk_score', 'month', 'weekday', 'quarter', 'year']\n",
      "   Special numeric columns: ['lead_time_days', 'route_risk_score']\n",
      "   Special categorical columns: None\n",
      "   Flag columns: None\n",
      "   General categorical columns: ['origin', 'destination', 'delivery_date', 'source', 'delay_severity']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ---------------------------\n",
    "# Column Groups\n",
    "# ---------------------------\n",
    "X_cols = X_train.columns if X_train is not None else []\n",
    "\n",
    "# Special categorical\n",
    "special_cat_cols = [c for c in [\"disruption_type\"] if c in X_cols]\n",
    "\n",
    "# Flag columns (binary indicators)\n",
    "flag_cols = [c for c in X_cols if c.lower() in [\"risk_flag\",\"disruption_flag\",\"incident_flag\"]]\n",
    "\n",
    "# Special numeric columns\n",
    "special_num_cols = [c for c in X_cols if c.lower() in [\"lead_time_days\", \"route_risk_score\"]]\n",
    "\n",
    "# General numeric (exclude special numeric, flags, regression target)\n",
    "num_cols = [c for c in X_train.select_dtypes(include=[\"int64\",\"float64\"]).columns \n",
    "            if c not in special_num_cols + flag_cols + ([reg_target] if reg_target else [])]\n",
    "\n",
    "# General categorical (exclude special categorical, flags, ID/date)\n",
    "non_features = [id_col, date_col]  # e.g., shipment_id, dispatch_date\n",
    "cat_cols = [c for c in X_train.select_dtypes(include=\"object\").columns \n",
    "            if c not in special_cat_cols + flag_cols + [col for col in non_features if col]]\n",
    "\n",
    "# ---------------------------\n",
    "# Pipelines\n",
    "# ---------------------------\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler(with_mean=False))\n",
    "]) if num_cols else None\n",
    "\n",
    "special_num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    (\"scaler\", StandardScaler(with_mean=False))\n",
    "]) if special_num_cols else None\n",
    "\n",
    "special_cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"none\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "]) if special_cat_cols else None\n",
    "\n",
    "flag_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0))\n",
    "]) if flag_cols else None\n",
    "\n",
    "general_cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "]) if cat_cols else None\n",
    "\n",
    "# ---------------------------\n",
    "# ColumnTransformer\n",
    "# ---------------------------\n",
    "transformers = []\n",
    "if num_cols and num_pipeline: transformers.append((\"num\", num_pipeline, num_cols))\n",
    "if special_num_cols and special_num_pipeline: transformers.append((\"special_num\", special_num_pipeline, special_num_cols))\n",
    "if special_cat_cols and special_cat_pipeline: transformers.append((\"special_cat\", special_cat_pipeline, special_cat_cols))\n",
    "if flag_cols and flag_pipeline: transformers.append((\"flags\", flag_pipeline, flag_cols))\n",
    "if cat_cols and general_cat_pipeline: transformers.append((\"general_cat\", general_cat_pipeline, cat_cols))\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Logging\n",
    "# ---------------------------\n",
    "print(\"✅ Preprocessor ready.\")\n",
    "print(\"   Numeric columns:\", num_cols or \"None\")\n",
    "print(\"   Special numeric columns:\", special_num_cols or \"None\")\n",
    "print(\"   Special categorical columns:\", special_cat_cols or \"None\")\n",
    "print(\"   Flag columns:\", flag_cols or \"None\")\n",
    "print(\"   General categorical columns:\", cat_cols or \"None\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cfeca3",
   "metadata": {},
   "source": [
    "Classification: Risk Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "39c95017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Merging rare classes into 'Other': ['Detour', 'Clear', 'Heavy', 'Shortage', 'Weather', 'Customs', 'Strike']\n",
      "Detected target 'disruption_type' with 4 unique classes.\n",
      "✅ Multi-class classification detected.\n",
      "✅ Class weights for Logistic Regression: {'High Risk': np.float64(0.36789886738606975), 'Low Risk': np.float64(2.7760767160161506), 'Moderate Risk': np.float64(1.7318429890848026), 'Other': np.float64(2.9051056338028167)}\n",
      "🔹 Training logreg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 5000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=5000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training rf...\n",
      "✅ Classification results saved with class imbalance handling!\n",
      "  logreg: F1-macro = 0.7583095045908608, Test AUC = 0.8667492331156597\n",
      "  rf: F1-macro = 0.4632209405501331, Test AUC = 1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "clf_results = {}\n",
    "\n",
    "# -------------------------------\n",
    "# Proceed only if classification target exists\n",
    "# -------------------------------\n",
    "if clf_target is not None and y_train is not None:\n",
    "\n",
    "    # -------------------------------\n",
    "    # Merge rare classes\n",
    "    # -------------------------------\n",
    "    class_counts = y_train.value_counts()\n",
    "    rare_classes = class_counts[class_counts < 1000].index.tolist()  # adjust threshold as needed\n",
    "    if rare_classes:\n",
    "        print(f\"⚠️ Merging rare classes into 'Other': {rare_classes}\")\n",
    "        y_train_proc = y_train.replace(rare_classes, \"Other\")\n",
    "        y_val_proc = y_val.replace(rare_classes, \"Other\")\n",
    "        y_test_proc = y_test.replace(rare_classes, \"Other\")\n",
    "    else:\n",
    "        y_train_proc, y_val_proc, y_test_proc = y_train.copy(), y_val.copy(), y_test.copy()\n",
    "\n",
    "    # -------------------------------\n",
    "    # Detect if binary or multi-class\n",
    "    # -------------------------------\n",
    "    classes = np.unique(y_train_proc)\n",
    "    is_binary = len(classes) == 2\n",
    "    print(f\"Detected target '{clf_target}' with {len(classes)} unique classes.\")\n",
    "    print(f\"✅ {'Binary' if is_binary else 'Multi-class'} classification detected.\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # Compute class weights for Logistic Regression\n",
    "    # -------------------------------\n",
    "    class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train_proc)\n",
    "    class_weight_dict = {cls: w for cls, w in zip(classes, class_weights)}\n",
    "    print(f\"✅ Class weights for Logistic Regression: {class_weight_dict}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # Ensure all splits have >1 class\n",
    "    # -------------------------------\n",
    "    skip_classification = any(len(np.unique(y)) < 2 for y in [y_train_proc, y_val_proc, y_test_proc])\n",
    "    if skip_classification:\n",
    "        print(\"⚠️ Skipping classification due to insufficient class diversity in splits.\")\n",
    "    else:\n",
    "\n",
    "        # -------------------------------\n",
    "        # Define pipelines\n",
    "        # -------------------------------\n",
    "        logreg = Pipeline([\n",
    "            (\"prep\", preprocessor),\n",
    "            (\"clf\", LogisticRegression(\n",
    "                max_iter=5000,\n",
    "                solver=\"lbfgs\",\n",
    "                multi_class=\"multinomial\",\n",
    "                class_weight=class_weight_dict\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        rf = Pipeline([\n",
    "            (\"prep\", preprocessor),\n",
    "            (\"clf\", RandomForestClassifier(\n",
    "                n_estimators=300,\n",
    "                max_depth=15,\n",
    "                min_samples_leaf=5,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        models = {\"logreg\": logreg, \"rf\": rf}\n",
    "\n",
    "        # -------------------------------\n",
    "        # Train, evaluate, and save models\n",
    "        # -------------------------------\n",
    "        for name, pipe in models.items():\n",
    "            print(f\"🔹 Training {name}...\")\n",
    "            pipe.fit(X_train, y_train_proc)\n",
    "\n",
    "            # Predictions\n",
    "            test_pred = pipe.predict(X_test)\n",
    "            val_pred = pipe.predict(X_val)\n",
    "\n",
    "            # Classification report & confusion matrix\n",
    "            report = classification_report(y_test_proc, test_pred, output_dict=True, zero_division=0)\n",
    "            cm = confusion_matrix(y_test_proc, test_pred).tolist()\n",
    "            f1_macro = f1_score(y_test_proc, test_pred, average=\"macro\")\n",
    "\n",
    "            # Multi-class AUC\n",
    "            try:\n",
    "                val_pred_prob = pipe.predict_proba(X_val)\n",
    "                test_pred_prob = pipe.predict_proba(X_test)\n",
    "                val_auc = roc_auc_score(y_val_proc, val_pred_prob, multi_class='ovr')\n",
    "                test_auc = roc_auc_score(y_test_proc, test_pred_prob, multi_class='ovr')\n",
    "            except Exception as e:\n",
    "                val_auc, test_auc = None, None\n",
    "                print(f\"⚠️ Could not compute multi-class AUC for {name}: {e}\")\n",
    "\n",
    "            clf_results[name] = {\n",
    "                \"f1_macro\": float(f1_macro),\n",
    "                \"val_auc\": float(val_auc) if val_auc is not None else None,\n",
    "                \"test_auc\": float(test_auc) if test_auc is not None else None,\n",
    "                \"report\": report,\n",
    "                \"cm\": cm\n",
    "            }\n",
    "\n",
    "            # Save model\n",
    "            os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "            joblib.dump(pipe, os.path.join(MODELS_DIR, f\"clf_{name}.joblib\"))\n",
    "\n",
    "        # Save metrics\n",
    "        os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "        with open(os.path.join(RESULTS_DIR, \"classification_results.json\"), \"w\") as f:\n",
    "            json.dump(clf_results, f, indent=2)\n",
    "\n",
    "        print(\"✅ Classification results saved with class imbalance handling!\")\n",
    "        for k, v in clf_results.items():\n",
    "            print(f\"  {k}: F1-macro = {v['f1_macro']}, Test AUC = {v['test_auc']}\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Skipping classification: no valid classification target detected or target is empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dddc454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
