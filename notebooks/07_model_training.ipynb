{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c59d053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes:\n",
      "Train: (24545, 16)\n",
      "Val: (5260, 16)\n",
      "Test: (5260, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shipment_id</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>dispatch_date</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>delay_days</th>\n",
       "      <th>disruption_type</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>source</th>\n",
       "      <th>lead_time_days</th>\n",
       "      <th>delay_severity</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>route_risk_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O1000</td>\n",
       "      <td>B33</td>\n",
       "      <td>S23</td>\n",
       "      <td>2023-10-27 00:00:00</td>\n",
       "      <td>2023-10-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>resilience</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Minor</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2023</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O1001</td>\n",
       "      <td>B1</td>\n",
       "      <td>S20</td>\n",
       "      <td>2023-07-08 00:00:00</td>\n",
       "      <td>2023-07-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>resilience</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Minor</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2023</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O1002</td>\n",
       "      <td>B2</td>\n",
       "      <td>S10</td>\n",
       "      <td>2023-12-29 00:00:00</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Shortage</td>\n",
       "      <td>1.0</td>\n",
       "      <td>resilience</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Severe</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2023</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O1003</td>\n",
       "      <td>B6</td>\n",
       "      <td>S10</td>\n",
       "      <td>2023-01-17 00:00:00</td>\n",
       "      <td>2023-01-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>resilience</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  shipment_id origin destination        dispatch_date delivery_date  \\\n",
       "0       O1000    B33         S23  2023-10-27 00:00:00    2023-10-28   \n",
       "1       O1001     B1         S20  2023-07-08 00:00:00    2023-07-09   \n",
       "2       O1002     B2         S10  2023-12-29 00:00:00    2024-01-07   \n",
       "3       O1003     B6         S10  2023-01-17 00:00:00    2023-01-20   \n",
       "\n",
       "   delay_days disruption_type  risk_score      source  lead_time_days  \\\n",
       "0         0.0             NaN         0.0  resilience             1.0   \n",
       "1         0.0             NaN         0.0  resilience             1.0   \n",
       "2         7.0        Shortage         1.0  resilience             9.0   \n",
       "3         0.0             NaN         0.0  resilience             3.0   \n",
       "\n",
       "  delay_severity  month  weekday  quarter  year  route_risk_score  \n",
       "0          Minor     10        4        4  2023               1.0  \n",
       "1          Minor      7        5        3  2023               1.0  \n",
       "2         Severe     12        4        4  2023               1.0  \n",
       "3       Moderate      1        1        1  2023               1.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PROCESSED = \"../data/processed/tabular\"\n",
    "MODELS_DIR = \"../models\"\n",
    "RESULTS_DIR = \"../results/metrics\"\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Files produced by 06_split_data.ipynb\n",
    "TRAIN_PATH = f\"{DATA_PROCESSED}/train.csv\"\n",
    "VAL_PATH   = f\"{DATA_PROCESSED}/val.csv\"\n",
    "TEST_PATH  = f\"{DATA_PROCESSED}/test.csv\"\n",
    "\n",
    "# Load all datasets\n",
    "df_train = pd.read_csv(TRAIN_PATH, low_memory=False)\n",
    "df_val   = pd.read_csv(VAL_PATH, low_memory=False)\n",
    "df_test  = pd.read_csv(TEST_PATH, low_memory=False)\n",
    "\n",
    "print(\"Data shapes:\")\n",
    "print(f\"Train: {df_train.shape}\")\n",
    "print(f\"Val: {df_val.shape}\")\n",
    "print(f\"Test: {df_test.shape}\")\n",
    "\n",
    "df_train.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e48a43a",
   "metadata": {},
   "source": [
    "Utility: Feature/Target Auto-Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f6b18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Multiple matches found: ['delay_days', 'lead_time_days', 'delay_severity']. Using the first one.\n",
      "⚠️ Multiple matches found: ['dispatch_date', 'delivery_date']. Using the first one.\n",
      "✅ Classification target: disruption_type\n",
      "✅ Regression target: delay_days\n",
      "✅ ID column: shipment_id\n",
      "✅ Date column: dispatch_date\n"
     ]
    }
   ],
   "source": [
    "def find_col(candidates, cols, col_map=None, return_all=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Return columns from candidates that exist in cols (case-insensitive).\n",
    "\n",
    "    Parameters:\n",
    "        candidates (list): List of candidate column names.\n",
    "        cols (list): List of columns to search in (lowercase).\n",
    "        col_map (dict, optional): Mapping from lowercase to original column names.\n",
    "        return_all (bool): If True, return all matches; else first match.\n",
    "        verbose (bool): If True, prints warnings for multiple matches.\n",
    "\n",
    "    Returns:\n",
    "        str or list: Matched column name(s) or None.\n",
    "    \"\"\"\n",
    "    matches = [c for c in candidates if c.lower() in cols]\n",
    "    if not matches:\n",
    "        return None\n",
    "    if return_all:\n",
    "        return [col_map[c.lower()] if col_map else c for c in matches]\n",
    "    if len(matches) > 1 and verbose:\n",
    "        print(f\"⚠️ Multiple matches found: {matches}. Using the first one.\")\n",
    "    return col_map[matches[0].lower()] if col_map else matches[0]\n",
    "\n",
    "\n",
    "# Lowercase lookup\n",
    "cols = [c.lower() for c in df_train.columns]\n",
    "col_map = {c.lower(): c for c in df_train.columns}\n",
    "\n",
    "# --- Classification target candidates ---\n",
    "classification_candidates = [\n",
    "    \"disruption_flag\",\"is_disrupted\",\"disrupted\",\"risk_flag\",\n",
    "    \"has_disruption\",\"disruption\",\"incident_flag\",\"disruption_type\"\n",
    "]\n",
    "clf_target = find_col(classification_candidates, cols, col_map=col_map)\n",
    "\n",
    "# --- Regression target candidates ---\n",
    "regression_candidates = [\n",
    "    \"delay_days\",\"delivery_delay_days\",\"delay\",\"days_delayed\",\n",
    "    \"delay_in_days\",\"lead_time_delays\",\"lead_time_days\",\"delay_severity\"\n",
    "]\n",
    "reg_target = find_col(regression_candidates, cols, col_map=col_map)\n",
    "\n",
    "# --- ID & Date candidates ---\n",
    "id_candidates = [\"shipment_id\",\"id\",\"order_id\",\"consignment_id\"]\n",
    "date_candidates = [\n",
    "    \"dispatch_date\",\"delivery_date\",\"ship_date\",\"event_time\",\n",
    "    \"timestamp\",\"created_at\",\"pickup_date\"\n",
    "]\n",
    "id_col   = find_col(id_candidates, cols, col_map=col_map)\n",
    "date_col = find_col(date_candidates, cols, col_map=col_map)\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"✅ Classification target:\", clf_target)\n",
    "print(\"✅ Regression target:\", reg_target)\n",
    "print(\"✅ ID column:\", id_col)\n",
    "print(\"✅ Date column:\", date_col)\n",
    "\n",
    "# --- Guardrails ---\n",
    "if clf_target is None and reg_target is None:\n",
    "    raise ValueError(\n",
    "        \"❌ No target columns detected. \"\n",
    "        \"Please set clf_target/reg_target manually.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f47c19",
   "metadata": {},
   "source": [
    "Split Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968379b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classification target (train) extracted: disruption_type, shape: (24545,)\n",
      "✅ Classification target (val) extracted: disruption_type, shape: (5260,)\n",
      "✅ Classification target (test) extracted: disruption_type, shape: (5260,)\n",
      "✅ Regression target (train) extracted: delay_days, shape: (24545,)\n",
      "✅ Regression target (val) extracted: delay_days, shape: (5260,)\n",
      "✅ Regression target (test) extracted: delay_days, shape: (5260,)\n",
      "\n",
      "✅ Features prepared\n",
      "   Numerical columns (7): ['risk_score', 'lead_time_days', 'month', 'weekday', 'quarter']...\n",
      "   Categorical columns (5): ['origin', 'destination', 'delivery_date', 'source', 'delay_severity']\n",
      "   Regression target: delay_days\n",
      "   Classification target: disruption_type\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Safe Drop of Non-Feature Columns\n",
    "# ---------------------------\n",
    "drop_cols = {id_col, date_col, clf_target, reg_target} - {None}\n",
    "\n",
    "def safe_drop(df, drop_cols):\n",
    "    \"\"\"Drop columns safely if they exist in DataFrame.\"\"\"\n",
    "    return df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "X_train_full = safe_drop(df_train, drop_cols)\n",
    "X_val_full   = safe_drop(df_val, drop_cols)\n",
    "X_test_full  = safe_drop(df_test, drop_cols)\n",
    "\n",
    "# ---------------------------\n",
    "# Extract Targets\n",
    "# ---------------------------\n",
    "def extract_target(df, target_col, name=\"target\"):\n",
    "    \"\"\"Extract target column from DataFrame if exists.\"\"\"\n",
    "    if target_col and target_col in df.columns:\n",
    "        y = df[target_col].copy()\n",
    "        print(f\"✅ {name} extracted: {target_col}, shape: {y.shape}\")\n",
    "        return y\n",
    "    print(f\"⚠️ {name} not found in DataFrame.\")\n",
    "    return None\n",
    "\n",
    "# Classification targets\n",
    "y_train_clf = extract_target(df_train, clf_target, \"Classification target (train)\")\n",
    "y_val_clf   = extract_target(df_val, clf_target, \"Classification target (val)\")\n",
    "y_test_clf  = extract_target(df_test, clf_target, \"Classification target (test)\")\n",
    "\n",
    "# Regression targets\n",
    "y_train_reg = extract_target(df_train, reg_target, \"Regression target (train)\")\n",
    "y_val_reg   = extract_target(df_val, reg_target, \"Regression target (val)\")\n",
    "y_test_reg  = extract_target(df_test, reg_target, \"Regression target (test)\")\n",
    "\n",
    "# ---------------------------\n",
    "# Detect Feature Types\n",
    "# ---------------------------\n",
    "num_cols = [c for c in X_train_full.columns if pd.api.types.is_numeric_dtype(X_train_full[c])]\n",
    "cat_cols = [c for c in X_train_full.columns if c not in num_cols]\n",
    "\n",
    "print(\"\\n✅ Features prepared\")\n",
    "print(f\"   Numerical columns ({len(num_cols)}): {num_cols[:5]}{'...' if len(num_cols) > 5 else ''}\")\n",
    "print(f\"   Categorical columns ({len(cat_cols)}): {cat_cols[:5]}{'...' if len(cat_cols) > 5 else ''}\")\n",
    "print(f\"   Regression target: {reg_target}\")\n",
    "print(f\"   Classification target: {clf_target}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bb9b58",
   "metadata": {},
   "source": [
    "Common Preprocess Pipline (Impute + Scale + One-Hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e6b11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def robust_stratified_split(X, y, test_size=0.3, val_size=0.5, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits X, y into train/val/test with:\n",
    "    - Imputation for missing values\n",
    "    - Stratified splitting if possible\n",
    "    - Falls back to random split if only one class\n",
    "    \"\"\"\n",
    "    # ------------------------\n",
    "    # Drop rows where target is NaN\n",
    "    # ------------------------\n",
    "    mask = y.notna()\n",
    "    X, y = X.loc[mask].copy(), y.loc[mask].copy()\n",
    "\n",
    "    # ------------------------\n",
    "    # Impute missing features\n",
    "    # ------------------------\n",
    "    # Numeric\n",
    "    num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    if len(num_cols) > 0:\n",
    "        num_imputer = SimpleImputer(strategy=\"median\")\n",
    "        X.loc[:, num_cols] = num_imputer.fit_transform(X[num_cols])\n",
    "\n",
    "    # Categorical\n",
    "    cat_cols = X.select_dtypes(include=\"object\").columns\n",
    "    if len(cat_cols) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        X.loc[:, cat_cols] = cat_imputer.fit_transform(X[cat_cols])\n",
    "\n",
    "    # ------------------------\n",
    "    # Encode categorical target for stratification if needed\n",
    "    # ------------------------\n",
    "    if y.dtype == object or str(y.dtype) == 'category':\n",
    "        y_strat = y.astype(\"category\").cat.codes\n",
    "    else:\n",
    "        y_strat = y.copy()\n",
    "\n",
    "    # ------------------------\n",
    "    # Check if target has at least 2 classes\n",
    "    # ------------------------\n",
    "    unique_classes = np.unique(y_strat)\n",
    "    stratify_possible = len(unique_classes) > 1\n",
    "\n",
    "    if not stratify_possible:\n",
    "        print(\"⚠️ Only one class present. Using random split instead of stratified.\")\n",
    "\n",
    "    # ------------------------\n",
    "    # Stratified or random train/temp split\n",
    "    # ------------------------\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y_strat if stratify_possible else None,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # ------------------------\n",
    "    # Stratified or random val/test split\n",
    "    # ------------------------\n",
    "    y_temp_strat = y_temp.astype(\"category\").cat.codes if (stratify_possible and y_temp.dtype == object) else y_temp\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size, stratify=y_temp_strat if stratify_possible else None,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # ------------------------\n",
    "    # Log class distributions\n",
    "    # ------------------------\n",
    "    print(\"Train classes:\", np.unique(y_train, return_counts=True))\n",
    "    print(\"Val classes:\", np.unique(y_val, return_counts=True))\n",
    "    print(\"Test classes:\", np.unique(y_test, return_counts=True))\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b70256d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classes: (array(['Clear', 'Customs', 'Detour', 'Heavy', 'High Risk', 'Low Risk',\n",
      "       'Moderate Risk', 'Shortage', 'Strike', 'Weather'], dtype=object), array([  230,   174,   241,   229, 11213,  1486,  2382,   189,   171,\n",
      "         186]))\n",
      "Val classes: (array(['Clear', 'Customs', 'Detour', 'Heavy', 'High Risk', 'Low Risk',\n",
      "       'Moderate Risk', 'Shortage', 'Strike', 'Weather'], dtype=object), array([  49,   37,   52,   49, 2403,  319,  510,   41,   36,   40]))\n",
      "Test classes: (array(['Clear', 'Customs', 'Detour', 'Heavy', 'High Risk', 'Low Risk',\n",
      "       'Moderate Risk', 'Shortage', 'Strike', 'Weather'], dtype=object), array([  49,   37,   52,   49, 2403,  318,  511,   40,   37,   40]))\n",
      "✅ Train/Val/Test split done\n",
      "Train shape: (16501, 15) | Val shape: (3536, 15) | Test shape: (3536, 15)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Prepare features and target\n",
    "# ------------------------\n",
    "if clf_target in df_train.columns:\n",
    "    X = df_train.drop(columns=[clf_target], errors=\"ignore\").copy()\n",
    "    y = df_train[clf_target].copy()\n",
    "else:\n",
    "    print(f\"⚠️ Classification target '{clf_target}' not found in df_train. Skipping split.\")\n",
    "    X = df_train.copy()\n",
    "    y = None\n",
    "\n",
    "# ------------------------\n",
    "# Perform robust stratified split if target is valid\n",
    "# ------------------------\n",
    "if y is not None and y.notna().any():\n",
    "    try:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = robust_stratified_split(X, y)\n",
    "        \n",
    "        # ------------------------\n",
    "        # Reset indices for convenience\n",
    "        # ------------------------\n",
    "        for df in [X_train, X_val, X_test, y_train, y_val, y_test]:\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        print(\"✅ Train/Val/Test split done\")\n",
    "        print(\"Train shape:\", X_train.shape, \"| Val shape:\", X_val.shape, \"| Test shape:\", X_test.shape)\n",
    "    except ValueError as e:\n",
    "        print(f\"⚠️ Skipping split: {e}\")\n",
    "        X_train, X_val, X_test = X.copy(), None, None\n",
    "        y_train, y_val, y_test = y.copy() if y is not None else None, None, None\n",
    "else:\n",
    "    print(\"⚠️ Skipping split: no valid target detected.\")\n",
    "    X_train, X_val, X_test = X.copy(), None, None\n",
    "    y_train, y_val, y_test = y.copy() if y is not None else None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbdb2892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessor ready.\n",
      "   Numeric columns: ['risk_score', 'month', 'weekday', 'quarter', 'year']\n",
      "   Special numeric columns: ['lead_time_days', 'route_risk_score']\n",
      "   Special categorical columns: None\n",
      "   Flag columns: None\n",
      "   General categorical columns: ['origin', 'destination', 'delivery_date', 'source', 'delay_severity']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ---------------------------\n",
    "# Column Groups\n",
    "# ---------------------------\n",
    "X_cols = X_train.columns if X_train is not None else []\n",
    "\n",
    "# Special categorical\n",
    "special_cat_cols = [c for c in [\"disruption_type\"] if c in X_cols]\n",
    "\n",
    "# Flag columns (binary indicators)\n",
    "flag_cols = [c for c in X_cols if c.lower() in [\"risk_flag\",\"disruption_flag\",\"incident_flag\"]]\n",
    "\n",
    "# Special numeric columns\n",
    "special_num_cols = [c for c in X_cols if c.lower() in [\"lead_time_days\", \"route_risk_score\"]]\n",
    "\n",
    "# General numeric (exclude special numeric, flags, regression target)\n",
    "num_cols = [c for c in X_train.select_dtypes(include=[\"int64\",\"float64\"]).columns \n",
    "            if c not in special_num_cols + flag_cols + ([reg_target] if reg_target else [])]\n",
    "\n",
    "# General categorical (exclude special categorical, flags, ID/date)\n",
    "non_features = [id_col, date_col]  # e.g., shipment_id, dispatch_date\n",
    "cat_cols = [c for c in X_train.select_dtypes(include=\"object\").columns \n",
    "            if c not in special_cat_cols + flag_cols + [col for col in non_features if col]]\n",
    "\n",
    "# ---------------------------\n",
    "# Pipelines\n",
    "# ---------------------------\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler(with_mean=False))\n",
    "]) if num_cols else None\n",
    "\n",
    "special_num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    (\"scaler\", StandardScaler(with_mean=False))\n",
    "]) if special_num_cols else None\n",
    "\n",
    "special_cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"none\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "]) if special_cat_cols else None\n",
    "\n",
    "flag_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0))\n",
    "]) if flag_cols else None\n",
    "\n",
    "general_cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "]) if cat_cols else None\n",
    "\n",
    "# ---------------------------\n",
    "# ColumnTransformer\n",
    "# ---------------------------\n",
    "transformers = []\n",
    "if num_cols and num_pipeline: transformers.append((\"num\", num_pipeline, num_cols))\n",
    "if special_num_cols and special_num_pipeline: transformers.append((\"special_num\", special_num_pipeline, special_num_cols))\n",
    "if special_cat_cols and special_cat_pipeline: transformers.append((\"special_cat\", special_cat_pipeline, special_cat_cols))\n",
    "if flag_cols and flag_pipeline: transformers.append((\"flags\", flag_pipeline, flag_cols))\n",
    "if cat_cols and general_cat_pipeline: transformers.append((\"general_cat\", general_cat_pipeline, cat_cols))\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Logging\n",
    "# ---------------------------\n",
    "print(\"✅ Preprocessor ready.\")\n",
    "print(\"   Numeric columns:\", num_cols or \"None\")\n",
    "print(\"   Special numeric columns:\", special_num_cols or \"None\")\n",
    "print(\"   Special categorical columns:\", special_cat_cols or \"None\")\n",
    "print(\"   Flag columns:\", flag_cols or \"None\")\n",
    "print(\"   General categorical columns:\", cat_cols or \"None\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cfeca3",
   "metadata": {},
   "source": [
    "Classification: Risk Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39c95017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Merging rare classes into 'Other': ['Detour', 'Clear', 'Heavy', 'Shortage', 'Weather', 'Customs', 'Strike']\n",
      "Detected target 'disruption_type' with 4 unique classes.\n",
      "✅ Multi-class classification detected.\n",
      "✅ Class weights for Logistic Regression: {'High Risk': np.float64(0.36789886738606975), 'Low Risk': np.float64(2.7760767160161506), 'Moderate Risk': np.float64(1.7318429890848026), 'Other': np.float64(2.9051056338028167)}\n",
      "🔹 Training logreg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 5000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=5000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training rf...\n",
      "✅ Classification results saved with class imbalance handling!\n",
      "  logreg: F1-macro = 0.7583095045908608, Test AUC = 0.8667492331156597\n",
      "  rf: F1-macro = 0.4632209405501331, Test AUC = 1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "clf_results = {}\n",
    "\n",
    "# -------------------------------\n",
    "# Proceed only if classification target exists\n",
    "# -------------------------------\n",
    "if clf_target is not None and y_train is not None:\n",
    "\n",
    "    # -------------------------------\n",
    "    # Merge rare classes\n",
    "    # -------------------------------\n",
    "    class_counts = y_train.value_counts()\n",
    "    rare_classes = class_counts[class_counts < 1000].index.tolist()  # adjust threshold as needed\n",
    "    if rare_classes:\n",
    "        print(f\"⚠️ Merging rare classes into 'Other': {rare_classes}\")\n",
    "        y_train_proc = y_train.replace(rare_classes, \"Other\")\n",
    "        y_val_proc = y_val.replace(rare_classes, \"Other\")\n",
    "        y_test_proc = y_test.replace(rare_classes, \"Other\")\n",
    "    else:\n",
    "        y_train_proc, y_val_proc, y_test_proc = y_train.copy(), y_val.copy(), y_test.copy()\n",
    "\n",
    "    # -------------------------------\n",
    "    # Detect if binary or multi-class\n",
    "    # -------------------------------\n",
    "    classes = np.unique(y_train_proc)\n",
    "    is_binary = len(classes) == 2\n",
    "    print(f\"Detected target '{clf_target}' with {len(classes)} unique classes.\")\n",
    "    print(f\"✅ {'Binary' if is_binary else 'Multi-class'} classification detected.\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # Compute class weights for Logistic Regression\n",
    "    # -------------------------------\n",
    "    class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train_proc)\n",
    "    class_weight_dict = {cls: w for cls, w in zip(classes, class_weights)}\n",
    "    print(f\"✅ Class weights for Logistic Regression: {class_weight_dict}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # Ensure all splits have >1 class\n",
    "    # -------------------------------\n",
    "    skip_classification = any(len(np.unique(y)) < 2 for y in [y_train_proc, y_val_proc, y_test_proc])\n",
    "    if skip_classification:\n",
    "        print(\"⚠️ Skipping classification due to insufficient class diversity in splits.\")\n",
    "    else:\n",
    "\n",
    "        # -------------------------------\n",
    "        # Define pipelines\n",
    "        # -------------------------------\n",
    "        logreg = Pipeline([\n",
    "            (\"prep\", preprocessor),\n",
    "            (\"clf\", LogisticRegression(\n",
    "                max_iter=5000,\n",
    "                solver=\"lbfgs\",\n",
    "                multi_class=\"multinomial\",\n",
    "                class_weight=class_weight_dict\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        rf = Pipeline([\n",
    "            (\"prep\", preprocessor),\n",
    "            (\"clf\", RandomForestClassifier(\n",
    "                n_estimators=300,\n",
    "                max_depth=15,\n",
    "                min_samples_leaf=5,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        models = {\"logreg\": logreg, \"rf\": rf}\n",
    "\n",
    "        # -------------------------------\n",
    "        # Train, evaluate, and save models\n",
    "        # -------------------------------\n",
    "        for name, pipe in models.items():\n",
    "            print(f\"🔹 Training {name}...\")\n",
    "            pipe.fit(X_train, y_train_proc)\n",
    "\n",
    "            # Predictions\n",
    "            test_pred = pipe.predict(X_test)\n",
    "            val_pred = pipe.predict(X_val)\n",
    "\n",
    "            # Classification report & confusion matrix\n",
    "            report = classification_report(y_test_proc, test_pred, output_dict=True, zero_division=0)\n",
    "            cm = confusion_matrix(y_test_proc, test_pred).tolist()\n",
    "            f1_macro = f1_score(y_test_proc, test_pred, average=\"macro\")\n",
    "\n",
    "            # Multi-class AUC\n",
    "            try:\n",
    "                val_pred_prob = pipe.predict_proba(X_val)\n",
    "                test_pred_prob = pipe.predict_proba(X_test)\n",
    "                val_auc = roc_auc_score(y_val_proc, val_pred_prob, multi_class='ovr')\n",
    "                test_auc = roc_auc_score(y_test_proc, test_pred_prob, multi_class='ovr')\n",
    "            except Exception as e:\n",
    "                val_auc, test_auc = None, None\n",
    "                print(f\"⚠️ Could not compute multi-class AUC for {name}: {e}\")\n",
    "\n",
    "            clf_results[name] = {\n",
    "                \"f1_macro\": float(f1_macro),\n",
    "                \"val_auc\": float(val_auc) if val_auc is not None else None,\n",
    "                \"test_auc\": float(test_auc) if test_auc is not None else None,\n",
    "                \"report\": report,\n",
    "                \"cm\": cm\n",
    "            }\n",
    "\n",
    "            # Save model\n",
    "            os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "            joblib.dump(pipe, os.path.join(MODELS_DIR, f\"clf_{name}.joblib\"))\n",
    "\n",
    "        # Save metrics\n",
    "        os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "        with open(os.path.join(RESULTS_DIR, \"classification_results.json\"), \"w\") as f:\n",
    "            json.dump(clf_results, f, indent=2)\n",
    "\n",
    "        print(\"✅ Classification results saved with class imbalance handling!\")\n",
    "        for k, v in clf_results.items():\n",
    "            print(f\"  {k}: F1-macro = {v['f1_macro']}, Test AUC = {v['test_auc']}\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Skipping classification: no valid classification target detected or target is empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3e6ebd",
   "metadata": {},
   "source": [
    "Delay Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dddc454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Encoded classes (multi-class): {'Minor delay': np.int64(0), 'Moderate delay': np.int64(1), 'On-time': np.int64(2), 'Severe delay': np.int64(3)}\n",
      "✅ Encoded classes (binary): {np.str_('Not Severe'): np.int64(0), np.str_('Severe'): np.int64(1)}\n",
      "\n",
      "=== Multi-class classification ===\n",
      "🔹 Training logreg (multi-class)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training rf (multi-class)...\n",
      "🔹 Training xgb (multi-class)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:19:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Binary classification (Severe vs Not Severe) ===\n",
      "🔹 Training logreg (binary)...\n",
      "🔹 Training rf (binary)...\n",
      "🔹 Training xgb (binary)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:20:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Hierarchical classification ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 2435 features, but RandomForestClassifier is expecting 2438 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 159\u001b[39m\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array(final_preds)\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# Evaluate on test\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m test_pred_hier = \u001b[43mhierarchical_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_full\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m clf_delay_results[\u001b[33m\"\u001b[39m\u001b[33mhierarchical\u001b[39m\u001b[33m\"\u001b[39m] = {\n\u001b[32m    161\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtest_report\u001b[39m\u001b[33m\"\u001b[39m: classification_report(y_test_c, test_pred_hier, output_dict=\u001b[38;5;28;01mTrue\u001b[39;00m, zero_division=\u001b[32m0\u001b[39m),\n\u001b[32m    162\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfusion_matrix\u001b[39m\u001b[33m\"\u001b[39m: confusion_matrix(y_test_c, test_pred_hier, labels=delay_le.classes_).tolist(),\n\u001b[32m    163\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mclasses\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(delay_le.classes_)\n\u001b[32m    164\u001b[39m }\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 149\u001b[39m, in \u001b[36mhierarchical_predict\u001b[39m\u001b[34m(X)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhierarchical_predict\u001b[39m(X):\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     preds_stage1 = \u001b[43mstage1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     final_preds = []\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(preds_stage1):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:789\u001b[39m, in \u001b[36mPipeline.predict\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m    787\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter(with_final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    788\u001b[39m         Xt = transform.transform(Xt)\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n\u001b[32m    792\u001b[39m routed_params = process_routing(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpredict\u001b[39m\u001b[33m\"\u001b[39m, **params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:903\u001b[39m, in \u001b[36mForestClassifier.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    883\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    884\u001b[39m \u001b[33;03m    Predict class for X.\u001b[39;00m\n\u001b[32m    885\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    901\u001b[39m \u001b[33;03m        The predicted classes.\u001b[39;00m\n\u001b[32m    902\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m     proba = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m:\n\u001b[32m    906\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classes_.take(np.argmax(proba, axis=\u001b[32m1\u001b[39m), axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:945\u001b[39m, in \u001b[36mForestClassifier.predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    943\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    944\u001b[39m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m945\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[32m    948\u001b[39m n_jobs, _, _ = _partition_estimators(\u001b[38;5;28mself\u001b[39m.n_estimators, \u001b[38;5;28mself\u001b[39m.n_jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:637\u001b[39m, in \u001b[36mBaseForest._validate_X_predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    635\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X.indices.dtype != np.intc \u001b[38;5;129;01mor\u001b[39;00m X.indptr.dtype != np.intc):\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2975\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AADITYA\\Desktop\\Supply Chain Risk Detection\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2839\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2840\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2841\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2842\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 2435 features, but RandomForestClassifier is expecting 2438 features as input."
     ]
    }
   ],
   "source": [
    "# Delay Classification (multi-class, binary, hierarchical)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "clf_delay_results = {}\n",
    "\n",
    "if reg_target is not None:\n",
    "    \n",
    "    # Bucketize delays into classes\n",
    "    \n",
    "    def bucketize_delay(y):\n",
    "        y = pd.to_numeric(y, errors=\"coerce\")\n",
    "        if pd.isna(y):\n",
    "            return \"On-time\"  # fallback\n",
    "        y = max(y, 0)\n",
    "        if y == 0:\n",
    "            return \"On-time\"\n",
    "        elif 1 <= y <= 3:\n",
    "            return \"Minor delay\"\n",
    "        elif 4 <= y <= 7:\n",
    "            return \"Moderate delay\"\n",
    "        else:\n",
    "            return \"Severe delay\"\n",
    "\n",
    "    y_train_c = y_train_reg.apply(bucketize_delay)\n",
    "    y_val_c   = y_val_reg.apply(bucketize_delay)\n",
    "    y_test_c  = y_test_reg.apply(bucketize_delay)\n",
    "\n",
    "    \n",
    "    # Option 1: Multi-class (original 4 classes)\n",
    "    \n",
    "    delay_le = LabelEncoder()\n",
    "    y_train_enc = delay_le.fit_transform(y_train_c)\n",
    "    y_val_enc   = delay_le.transform(y_val_c)\n",
    "    y_test_enc  = delay_le.transform(y_test_c)\n",
    "\n",
    "    print(\"✅ Encoded classes (multi-class):\", dict(zip(delay_le.classes_, delay_le.transform(delay_le.classes_))))\n",
    "\n",
    "    \n",
    "    # Option 2: Binary collapse (Severe vs Not-Severe)\n",
    "    \n",
    "    def collapse_binary(y):\n",
    "        return np.where(y == \"Severe delay\", \"Severe\", \"Not Severe\")\n",
    "\n",
    "    y_train_bin = collapse_binary(y_train_c)\n",
    "    y_val_bin   = collapse_binary(y_val_c)\n",
    "    y_test_bin  = collapse_binary(y_test_c)\n",
    "\n",
    "    bin_le = LabelEncoder()\n",
    "    y_train_bin_enc = bin_le.fit_transform(y_train_bin)\n",
    "    y_val_bin_enc   = bin_le.transform(y_val_bin)\n",
    "    y_test_bin_enc  = bin_le.transform(y_test_bin)\n",
    "\n",
    "    print(\"✅ Encoded classes (binary):\", dict(zip(bin_le.classes_, bin_le.transform(bin_le.classes_))))\n",
    "\n",
    "    \n",
    "    # Pipelines (shared)\n",
    "    \n",
    "    def get_models(num_classes):\n",
    "        return {\n",
    "            \"logreg\": Pipeline([\n",
    "                (\"prep\", preprocessor),\n",
    "                (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42))\n",
    "            ]),\n",
    "            \"rf\": Pipeline([\n",
    "                (\"prep\", preprocessor),\n",
    "                (\"clf\", RandomForestClassifier(\n",
    "                    n_estimators=300, max_depth=15, class_weight=\"balanced\",\n",
    "                    random_state=42, n_jobs=-1\n",
    "                ))\n",
    "            ]),\n",
    "            \"xgb\": Pipeline([\n",
    "                (\"prep\", preprocessor),\n",
    "                (\"clf\", XGBClassifier(\n",
    "                    objective=\"multi:softprob\" if num_classes > 2 else \"binary:logistic\",\n",
    "                    num_class=num_classes if num_classes > 2 else None,\n",
    "                    eval_metric=\"mlogloss\", use_label_encoder=False,\n",
    "                    random_state=42, n_jobs=-1\n",
    "                ))\n",
    "            ])\n",
    "        }\n",
    "\n",
    "    \n",
    "    # Train + evaluate (Multi-class)\n",
    "    \n",
    "    print(\"\\n=== Multi-class classification ===\")\n",
    "    models_mc = get_models(len(delay_le.classes_))\n",
    "    for name, pipe in models_mc.items():\n",
    "        print(f\"🔹 Training {name} (multi-class)...\")\n",
    "        pipe.fit(X_train_full, y_train_enc)\n",
    "\n",
    "        val_pred = delay_le.inverse_transform(pipe.predict(X_val_full))\n",
    "        test_pred = delay_le.inverse_transform(pipe.predict(X_test_full))\n",
    "\n",
    "        clf_delay_results[f\"{name}_multi\"] = {\n",
    "            \"val_report\": classification_report(y_val_c, val_pred, output_dict=True, zero_division=0),\n",
    "            \"test_report\": classification_report(y_test_c, test_pred, output_dict=True, zero_division=0),\n",
    "            \"confusion_matrix\": confusion_matrix(y_test_c, test_pred, labels=delay_le.classes_).tolist(),\n",
    "            \"classes\": list(delay_le.classes_)\n",
    "        }\n",
    "\n",
    "    \n",
    "    # Train + evaluate (Binary)\n",
    "    \n",
    "    print(\"\\n=== Binary classification (Severe vs Not Severe) ===\")\n",
    "    models_bin = get_models(len(bin_le.classes_))\n",
    "    for name, pipe in models_bin.items():\n",
    "        print(f\"🔹 Training {name} (binary)...\")\n",
    "        pipe.fit(X_train_full, y_train_bin_enc)\n",
    "\n",
    "        val_pred = bin_le.inverse_transform(pipe.predict(X_val_full))\n",
    "        test_pred = bin_le.inverse_transform(pipe.predict(X_test_full))\n",
    "\n",
    "        clf_delay_results[f\"{name}_binary\"] = {\n",
    "            \"val_report\": classification_report(y_val_bin, val_pred, output_dict=True, zero_division=0),\n",
    "            \"test_report\": classification_report(y_test_bin, test_pred, output_dict=True, zero_division=0),\n",
    "            \"confusion_matrix\": confusion_matrix(y_test_bin, test_pred, labels=bin_le.classes_).tolist(),\n",
    "            \"classes\": list(bin_le.classes_)\n",
    "        }\n",
    "\n",
    "    # Option 3: Hierarchical classification\n",
    "\n",
    "    print(\"\\n=== Hierarchical classification ===\")\n",
    "\n",
    "    # Stage 1: Severe vs Not-Severe\n",
    "    stage1 = Pipeline([\n",
    "        (\"prep\", preprocessor),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=2000, class_weight=\"balanced\", random_state=42))\n",
    "    ])\n",
    "    stage1.fit(X_train_full, y_train_bin)\n",
    "\n",
    "\n",
    "    # Stage 2: Only on Not Severe subset\n",
    "    mask_train = (y_train_bin == \"Not Severe\")\n",
    "    stage2 = Pipeline([\n",
    "        (\"prep\", preprocessor),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=200, class_weight=\"balanced\", random_state=42))\n",
    "    ])\n",
    "    stage2.fit(X_train_full[mask_train], y_train_c[mask_train])\n",
    "\n",
    "\n",
    "    def hierarchical_predict(X):\n",
    "        preds_stage1 = stage1.predict(X)\n",
    "        final_preds = []\n",
    "        for i, p in enumerate(preds_stage1):\n",
    "            if p == \"Severe\":\n",
    "                final_preds.append(\"Severe delay\")\n",
    "            else:\n",
    "                final_preds.append(stage2.predict(X[i].reshape(1, -1))[0])\n",
    "        return np.array(final_preds)\n",
    "\n",
    "    # Evaluate on test\n",
    "    test_pred_hier = hierarchical_predict(X_test_full)\n",
    "    clf_delay_results[\"hierarchical\"] = {\n",
    "        \"test_report\": classification_report(y_test_c, test_pred_hier, output_dict=True, zero_division=0),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test_c, test_pred_hier, labels=delay_le.classes_).tolist(),\n",
    "        \"classes\": list(delay_le.classes_)\n",
    "    }\n",
    "\n",
    "    # Save results\n",
    "\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    with open(os.path.join(RESULTS_DIR, \"delay_classification_results.json\"), \"w\") as f:\n",
    "        json.dump(clf_delay_results, f, indent=2)\n",
    "\n",
    "    print(\"✅ Delay classification results saved (multi-class, binary, hierarchical).\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping delay classification: no target detected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9dd12",
   "metadata": {},
   "source": [
    "Anomaly Detection (IsolationForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "659c0185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Anomaly detection summary: {'threshold_percentile': 5, 'threshold_value': -0.5977612512504518, 'anomalies_detected': 177, 'total_test': 3536, 'rate': 0.05005656108597285}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "anomly_report = None\n",
    "\n",
    "if len(num_cols) > 0:\n",
    "    iso_pipeline = Pipeline([\n",
    "        (\"num_only\", ColumnTransformer([(\"num\", num_pipeline, num_cols)], remainder=\"drop\")),\n",
    "        (\"iso\", IsolationForest(\n",
    "            n_estimators=300,\n",
    "            contamination=\"auto\",\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    iso_pipeline.fit(X_train)\n",
    "\n",
    "    test_scores = iso_pipeline[\"iso\"].score_samples(iso_pipeline[\"num_only\"].transform(X_test))\n",
    "\n",
    "    # Define threshold at 5th percentile\n",
    "    anomaly_threshold = np.percentile(test_scores, 5)  \n",
    "    anomalies = (test_scores <= anomaly_threshold).astype(int)\n",
    "\n",
    "    anomaly_report = {\n",
    "        \"threshold_percentile\": 5,\n",
    "        \"threshold_value\": float(anomaly_threshold),\n",
    "        \"anomalies_detected\": int(anomalies.sum()),\n",
    "        \"total_test\": int(len(anomalies)),\n",
    "        \"rate\": float(anomalies.mean())\n",
    "    }\n",
    "\n",
    "    # Save results + trained pipeline\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    with open(os.path.join(RESULTS_DIR, \"anomaly_results.json\"), \"w\") as f:\n",
    "        json.dump(anomaly_report, f, indent=2)\n",
    "\n",
    "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "    joblib.dump(iso_pipeline, os.path.join(MODELS_DIR, \"anomaly_isolation_forest.joblib\"))\n",
    "\n",
    "    print(\"✅ Anomaly detection summary:\", anomaly_report)\n",
    "else:\n",
    "    print(\"⚠️ Skipping anomaly detection: no numeric columns found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8804a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
